{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Analysis \n",
    "import matplotlib.pyplot as plt #Visulization\n",
    "import seaborn as sns #Visulization\n",
    "import numpy as np #Analysis \n",
    "from scipy.stats import norm #Analysis \n",
    "from sklearn.preprocessing import StandardScaler #Analysis \n",
    "from scipy import stats #Analysis \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test_idx = test['key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리 준비작업\n",
    "- 인천 수정\n",
    "- Validation 구축 : Test가 가장 마지막 거래로만 이루어져 있어서, 실제 제출 전 점수를 평가 할 validation도 비슷하게 구축. **(주의 : 샘플링에 의해 Validation Score는 달라지기 때문에 동일한 샘플링 기법을 적용한것 끼리 비교해야 함 !!!)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인천의 경우 서울보다는 부산에 가까워서 city를 부산으로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test['key']==1503614,'city'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 시간의 순서대로 이루어져 있어서 Merge과정에서 순서가 깨지지 않도록 index컬럼을 생성해서 sort작업을 진행해줄것임. \n",
    "- 이를 안해주면 fold에서 다른 cv값이 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_len = train.shape[0]\n",
    "df_all = pd.concat([train,test])\n",
    "\n",
    "index = []\n",
    "for i in range(0,df_all.shape[0]):\n",
    "    index.append(i)\n",
    "df_all['index'] = index\n",
    "\n",
    "train = df_all[:train_len].reset_index(drop=True)\n",
    "df_test = df_all[train_len:].reset_index(drop=True)\n",
    "\n",
    "df_train_busan = train[train['city']==0]\n",
    "df_test_busan = df_test[df_test['city']==0]\n",
    "df_train_seoul = train[train['city']==1]\n",
    "df_test_seoul = df_test[df_test['city']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신뢰성 있는 Validation 구축을 위해서 아파트 별로 가장 마지막 거래를 Validation으로 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "busan_val_idx = df_train_busan.groupby(['apartment_id']).tail(1).index\n",
    "busan_valid = df_train_busan.loc[busan_val_idx,:]\n",
    "busan_valid = busan_valid[['key','transaction_real_price']]\n",
    "\n",
    "seoul_val_idx = df_train_seoul.groupby(['apartment_id']).tail(1).index\n",
    "seoul_valid = df_train_seoul.loc[seoul_val_idx,:]\n",
    "seoul_valid = seoul_valid[['key','transaction_real_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1277 2629\n"
     ]
    }
   ],
   "source": [
    "print(busan_valid.shape[0],seoul_valid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_train_busan,df_train_seoul,df_test_busan,df_test_seoul\n",
    "del df_test,train,df_all\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "- 트레인은 날짜가 고른 반면, 테스트는 2018년도 6월 이후가 압도적으로 많음. 샘플링 작업이 필요.\n",
    "- 방과 화장실 0인 값 대체 : 동일한 아파트에서 비슷한 크기에 값이 존재하면 그로 채워넣고, 그렇지 않으면 비슷한 크기에서 median으로 채워 넣음. \n",
    "- 방과 화장실 결측치 대체 : 동일한 아파트에서 비슷한 크기에 값이 존재하면 그로 채워넣고, 그렇지 않으면 비슷한 크기에서 median으로 채워 넣음. \n",
    "- 주차장의 결측치는 0으로 대체\n",
    "- 난방과 현관구조는 None으로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test_idx = test['key']\n",
    "test.loc[test['key']==1503614,'city'] = 0\n",
    "\n",
    "train_up1 = train[train['transaction_year_month']>201806]\n",
    "train_up1['transaction_real_price'] = train_up1['transaction_real_price'] + 10000000\n",
    "\n",
    "train_up2 = train[train['transaction_year_month']>201806]\n",
    "train_up2['transaction_real_price'] = train_up2['transaction_real_price'] + 5000000\n",
    "\n",
    "train = pd.concat([train,train_up1])\n",
    "train = pd.concat([train,train_up2])\n",
    "train = train.reset_index(drop=True)\n",
    "del train_up1,train_up2\n",
    "gc.collect()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_len = train.shape[0]\n",
    "df_all = pd.concat([train,test])\n",
    "\n",
    "index = []\n",
    "for i in range(0,df_all.shape[0]):\n",
    "    index.append(i)\n",
    "df_all['index'] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 0인 값 대체\n",
    "df_all.loc[(df_all['apartment_id']==2805) & (df_all['supply_area'] > 90),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==2805) & (df_all['supply_area'] > 90),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==895) & (df_all['supply_area'] > 137),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==895) & (df_all['supply_area'] > 137),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==903) & (df_all['supply_area'] > 135),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==903) & (df_all['supply_area'] > 135),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==1622) & (df_all['supply_area'] == 127.07),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==1622) & (df_all['supply_area'] == 127.07),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==14029) & (df_all['supply_area'] > 100),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==14029) & (df_all['supply_area'] > 100),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==14029) & (df_all['supply_area'] < 100),'room_count'] = 2\n",
    "df_all.loc[(df_all['apartment_id']==14029) & (df_all['supply_area'] < 100),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==12067) & (df_all['supply_area'] >= 95),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==12067) & (df_all['supply_area'] >= 95),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==12067) & (df_all['supply_area'] == 92),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==12067) & (df_all['supply_area'] == 92),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==12067) & (df_all['supply_area'] < 90),'room_count'] = 2\n",
    "df_all.loc[(df_all['apartment_id']==12067) & (df_all['supply_area'] < 90),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==360) & (df_all['supply_area'] == 189.99),'room_count'] = 5\n",
    "df_all.loc[(df_all['apartment_id']==360) & (df_all['supply_area'] == 189.99),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==360) & (df_all['supply_area'] == 154.46),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==360) & (df_all['supply_area'] == 154.46),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==568) & (df_all['supply_area']//10 == 11.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==568) & (df_all['supply_area']//10 == 11.0),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==1543) & (df_all['supply_area'] > 150 ),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==1543) & (df_all['supply_area'] > 150),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==618) & (df_all['supply_area'] == 92.94),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==618) & (df_all['supply_area'] == 92.94),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==618) & (df_all['supply_area'] == 110.57),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==618) & (df_all['supply_area'] == 110.57),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==4368) & (df_all['supply_area'] > 90),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==4368) & (df_all['supply_area'] > 90),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==4368) & (df_all['supply_area'] < 90),'room_count'] = 2\n",
    "df_all.loc[(df_all['apartment_id']==4368) & (df_all['supply_area'] < 90),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==3701) & (df_all['supply_area'] == 148.55),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==3701) & (df_all['supply_area'] == 148.55),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==346) & (df_all['supply_area'] > 100),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==346) & (df_all['supply_area'] > 100),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==1524) & (df_all['supply_area'] == 104.39),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==1524) & (df_all['supply_area'] == 104.39),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==1524) & (df_all['supply_area'] == 175.60),'room_count'] = 5\n",
    "df_all.loc[(df_all['apartment_id']==1524) & (df_all['supply_area'] == 175.60),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 9.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 9.0),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 10.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 10.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 13.0),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 13.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 14.0),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 14.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==1112) & (df_all['supply_area']//10 == 7.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==1112) & (df_all['supply_area']//10 == 7.0),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==65) & (df_all['supply_area']//10 == 11.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==65) & (df_all['supply_area']//10 == 11.0),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==541) & (df_all['supply_area']//10 == 8.0),'room_count'] = 2\n",
    "df_all.loc[(df_all['apartment_id']==541) & (df_all['supply_area']//10 == 8.0),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==184) & (df_all['supply_area'] == 66.12),'room_count'] = 2\n",
    "df_all.loc[(df_all['apartment_id']==184) & (df_all['supply_area'] == 66.12),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==2601) & (df_all['supply_area'] == 104.97),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==2601) & (df_all['supply_area'] == 104.97),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==6161) & (df_all['supply_area'] == 99.91),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==6161) & (df_all['supply_area'] == 99.91),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==3685) & (df_all['supply_area'] == 115.70),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==3685) & (df_all['supply_area'] == 115.70),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==21288) & (df_all['supply_area'] == 116.03),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==21288) & (df_all['supply_area'] == 116.03),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==10636) & (df_all['supply_area'] == 112.40),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==10636) & (df_all['supply_area'] == 112.40),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==1162) & (df_all['supply_area'] == 154.71),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==1162) & (df_all['supply_area'] == 154.71),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==10989) & (df_all['supply_area'] == 110.51),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==10989) & (df_all['supply_area'] == 110.51),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==11096) & (df_all['supply_area'] == 97.09),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==11096) & (df_all['supply_area'] == 97.09),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==184) & (df_all['supply_area'] == 69.42),'room_count'] = 2\n",
    "df_all.loc[(df_all['apartment_id']==184) & (df_all['supply_area'] == 69.42),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==534) & (df_all['supply_area'] //10 == 11.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==534) & (df_all['supply_area'] //10 == 11.0),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==17384) & (df_all['supply_area'] //10 == 11.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==17384) & (df_all['supply_area'] //10 == 11.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area'] //10 == 10.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area'] //10 == 10.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==4058) & (df_all['supply_area'] //10 == 9.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==4058) & (df_all['supply_area'] //10 == 9.0),'bathroom_count'] = 2\n",
    "\n",
    "#df_all.loc[(df_all['apartment_id']==1388) & (df_all['room_count']==0)]\n",
    "df_all.loc[(df_all['apartment_id']==1388) & (df_all['supply_area'] //10 == 14.0),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==1388) & (df_all['supply_area'] //10 == 14.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==7136) & (df_all['supply_area'] //10 == 7.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==7136) & (df_all['supply_area'] //10 == 7.0),'bathroom_count'] = 1\n",
    "\n",
    "#df_all.loc[(df_all['apartment_id']==18737)]\n",
    "df_all.loc[(df_all['apartment_id']==18737) & (df_all['supply_area'] //10 == 17.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==18737) & (df_all['supply_area'] //10 == 17.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==18741) & (df_all['supply_area'] >= 160),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==18741) & (df_all['supply_area'] >= 160),'bathroom_count'] = 2\n",
    "\n",
    "#df_all.loc[(df_all['apartment_id']==18732)]\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==18732) & (df_all['supply_area'] //10 == 11.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==18732) & (df_all['supply_area'] //10 == 11.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==18732) & (df_all['supply_area'] //10 == 18.0),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==18732) & (df_all['supply_area'] //10 == 18.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==360) & (df_all['supply_area'] //10 == 16.0),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==360) & (df_all['supply_area'] //10 == 16.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==8460) & (df_all['supply_area'] //10 == 8.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==8460) & (df_all['supply_area'] //10 == 8.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==6175) & (df_all['supply_area'] > 290),'room_count'] = 5\n",
    "df_all.loc[(df_all['apartment_id']==6175) & (df_all['supply_area'] > 290),'bathroom_count'] = 3\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==6232)]\n",
    "df_all.loc[(df_all['apartment_id']==6232) & (df_all['supply_area'] //10 == 19.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==6232) & (df_all['supply_area'] //10 == 19.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==15502) & (df_all['supply_area'] //10 == 10.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==15502) & (df_all['supply_area'] //10 == 10.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==568) & (df_all['supply_area'] //10 == 19.0),'room_count'] = 5\n",
    "df_all.loc[(df_all['apartment_id']==568) & (df_all['supply_area'] //10 == 19.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==16837) & (df_all['supply_area'] //10 == 9.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==16837) & (df_all['supply_area'] //10 == 9.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==37468) & (df_all['supply_area']  <= 200),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==37468) & (df_all['supply_area']  <= 200),'bathroom_count'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결측치 대체\n",
    "- 방, 화장실은 위와 동일한 방식으로 채워넣음.\n",
    "- 주차장의 경우 0으로 대체. dacon에 물어본 결과 결측치는 0이라고 했음.\n",
    "- 히트 및 현관의 결측치는 None으로 대체. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 방과 화장실 결측치\n",
    "df_all.loc[df_all['apartment_id'] == 9005, ['room_count']] = 1\n",
    "df_all.loc[df_all['apartment_id'] == 9005, ['bathroom_count']] = 1\n",
    "\n",
    "df_all.loc[df_all['apartment_id'] == 1179, ['room_count']] = 4\n",
    "df_all.loc[df_all['apartment_id'] == 1179, ['bathroom_count']] = 2\n",
    "\n",
    "df_all.loc[df_all['apartment_id'] == 10627, ['room_count']] = 3\n",
    "df_all.loc[df_all['apartment_id'] == 10627, ['bathroom_count']] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id'] == 10627) & (df_all['supply_area'] == 56.61), ['room_count']] = 2\n",
    "df_all.loc[(df_all['apartment_id'] == 10627) & (df_all['supply_area'] == 56.61), ['bathroom_count']] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id'] == 7992) , ['room_count']] = 3\n",
    "df_all.loc[(df_all['apartment_id'] == 7992) & (df_all['supply_area'] <= 81), ['bathroom_count']] = 1\n",
    "df_all.loc[(df_all['apartment_id'] == 7992) & (df_all['supply_area'] > 81), ['bathroom_count']] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id'] == 7118) & (df_all['supply_area'] == 75.55), ['room_count']] = 3\n",
    "df_all.loc[(df_all['apartment_id'] == 7118) & (df_all['supply_area'] == 75.55), ['bathroom_count']] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id'] == 7118) & (df_all['supply_area']//10 == 5.0), ['room_count']] = 2\n",
    "df_all.loc[(df_all['apartment_id'] == 7118) & (df_all['supply_area']//10 == 5.0), ['bathroom_count']] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id'] == 4047) & (df_all['supply_area']//10 == 11.0), ['room_count']] = 3\n",
    "df_all.loc[(df_all['apartment_id'] == 4047) & (df_all['supply_area']//10 == 11.0), ['bathroom_count']] =2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id'] == 37175) & (df_all['supply_area'] < 80), ['room_count']] = 2\n",
    "df_all.loc[(df_all['apartment_id'] == 37175) & (df_all['supply_area'] < 80), ['bathroom_count']] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id'] == 37175) & (df_all['supply_area'] > 80), ['room_count']] = 3\n",
    "df_all.loc[(df_all['apartment_id'] == 37175) & (df_all['supply_area'] > 80), ['bathroom_count']] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 주차장 결측치\n",
    "df_all.loc[(df_all['total_parking_capacity_in_site'].isnull()), ['total_parking_capacity_in_site']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 히트 결측치\n",
    "df_all.loc[(df_all['heat_type'].isnull()), ['heat_type']] = 'None'\n",
    "df_all.loc[(df_all['heat_fuel'].isnull()), ['heat_fuel']] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 현관구조 결측치\n",
    "df_all.loc[(df_all['heat_fuel'].isnull()), ['front_door_structure']] = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파생변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 용적률(容積率)은 건축 용어로 전체 대지면적에 대한 건물 연면적의 비율을 뜻하며 백분율로 표시한다. \n",
    "### 용적률이 높을수록 건축할 수 있는 연면적이 많아져 건축밀도가 높아지므로, 적정 주거환경을 보장하기 위하여 용적률의 상한선을 지정한다.\n",
    "df_all['effective_ratio'] = (df_all['exclusive_use_area'] / df_all['supply_area']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 시간을 좀 더 세부적으로 나타냄. \n",
    "df_all['transaction_date1'] = df_all.transaction_date.apply(lambda x: x[-2:])\n",
    "#convert int to date\n",
    "df_all['transaction_year_month1'] = df_all['transaction_year_month'].astype(str)\n",
    "#join month and date \n",
    "df_all['transaction_year_month_date'] = df_all[['transaction_year_month1', 'transaction_date1']].apply(lambda x: ''.join(x), axis=1)\n",
    "#convert  month and date to datetime \n",
    "df_all['transaction_year_month_date'] = pd.to_datetime(df_all['transaction_year_month_date'] )\n",
    "#reindext datetime\n",
    "del df_all['transaction_date1']; del df_all['transaction_year_month1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 계산의 편의성을 위해 날짜를 만들어 둠.\n",
    "df_all['year'] = df_all['transaction_year_month_date'].dt.year\n",
    "df_all['month'] = df_all['transaction_year_month_date'].dt.month\n",
    "df_all['month'] = df_all['month'].apply(lambda x : x-1 if x%2 == 0 else x)\n",
    "df_all['mean_year_month'] = df_all['year']*100 + df_all['month']\n",
    "del df_all['year']\n",
    "del df_all['month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 동일한 아파트의 가장 최근의 거래와 현재 거래의 차이를 계산\n",
    "df_all['last_month'] = df_all.groupby(['apartment_id'])['transaction_year_month'].shift(1)\n",
    "df_all['diff_month'] = df_all['transaction_year_month'] - df_all['last_month']\n",
    "del df_all['last_month']\n",
    "\n",
    "### 방의 총 갯수 ( 화장실 + 방 )\n",
    "df_all['total_room'] = df_all['room_count'] + df_all['bathroom_count']\n",
    "\n",
    "### Last_price_1과 3은 동일한 아파트의 면적대비 거래액을 의미. 추가로 현재 면적을 곱해줘야 함. \n",
    "df_all['last_price_1'] = df_all['transaction_real_price']/df_all['exclusive_use_area']\n",
    "df_all['last_price_1'] = df_all.groupby(['apartment_id'])['last_price_1'].shift(1)\n",
    "\n",
    "df_all['last_price_3'] = df_all['transaction_real_price']/df_all['supply_area']\n",
    "df_all['last_price_3'] = df_all.groupby(['apartment_id'])['last_price_3'].shift(1)\n",
    "\n",
    "df_all['last_area'] = df_all.groupby(['apartment_id'])['exclusive_use_area'].shift(1)\n",
    "df_all['last_transaction_year_month'] = df_all.groupby(['apartment_id'])['transaction_year_month'].shift(1)\n",
    "#df_all['transaction_real_price'] = np.log1p(df_all['transaction_real_price'])\n",
    "\n",
    "df_all['last_price_1'] = df_all['last_price_1'] * df_all['exclusive_use_area'] #현재 면적을 곱해줘서 비교를 가능하게 만듬.\n",
    "df_all['last_price_3'] = df_all['last_price_3'] * df_all['supply_area'] #현재 면적을 곱해줘서 비교를 가능하게 만듬.\n",
    "del df_all['last_area'],df_all['last_transaction_year_month']\n",
    "\n",
    "### log를 씌어줘서 정규성을 띄게 만듬. \n",
    "df_all['last_price_1'] = np.log1p(df_all['last_price_1'])\n",
    "df_all['last_price_3'] = np.log1p(df_all['last_price_3'])\n",
    "df_all['transaction_real_price'] = np.log1p(df_all['transaction_real_price'])\n",
    "\n",
    "### 빌딩의 간격계산\n",
    "df_all['difference_building_height'] = df_all['tallest_building_in_sites'] - df_all['lowest_building_in_sites']\n",
    "### 세대당 주차수 계산\n",
    "df_all['capacity_per_household'] = df_all['total_parking_capacity_in_site']/df_all['total_household_count_in_sites']\n",
    "\n",
    "### 아파트당 세대 수 계산\n",
    "df_all['household_per_building'] = df_all['total_household_count_in_sites']/df_all['apartment_building_count_in_sites']\n",
    "\n",
    "### 아파트당 타입의 비율 계산\n",
    "df_all['areahousehold_per_household'] = df_all['total_household_count_of_area_type']/df_all['total_household_count_in_sites']\n",
    "\n",
    "df_all['year'] = df_all['transaction_year_month']//100\n",
    "\n",
    "### 거래된 기간과 완성된 년도의 차이 계산\n",
    "df_all['transaction_diff_completion'] = df_all['transaction_year_month'] - df_all['year_of_completion']\n",
    "\n",
    "### 몇번째 층인지 비율 계산\n",
    "df_all['floor_ratio'] = df_all['floor']/df_all['tallest_building_in_sites']\n",
    "\n",
    "### 재개발 예정인지 가중치 줌. \n",
    "### 35를 상한선으로 잡은것은 이 이상이 되면 재개발 될 거라는 심리가 떨어져서 임.\n",
    "df_all['weight'] = 0\n",
    "df_all.loc[((df_all['year']-df_all['year_of_completion']) >= 25) & ((df_all['year']-df_all['year_of_completion']) < 35) & (df_all['effective_ratio'] >= 80) & (df_all['tallest_building_in_sites'] <=5),'weight'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subway, school 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 아래의 파일은 따로 첨부한 코드에 계산식이 나와있습니다.\n",
    "### Apartment_subway : 0.5, 1km내에 몇개의 지하철이 있냐, 몇개의 호선이 있냐\n",
    "### Apartment_gd_hd : 강남 및 해운대로부터의 거리가 얼마나 되냐\n",
    "### apartment_school : 0.5km 내에 초,중,고등학교가 있냐 없냐. total_0.5는 3개 중에서 몇개가 있는지\n",
    "### min_distance_apartment : 가장 가까운 초,중,고,지하철의 거리\n",
    "apartment = pd.read_csv(\"Apartment_subway.csv\")\n",
    "apartment1 = apartment[['apartment_id','subwayline_count_0.5','subwayline_count_1','subway_count_0.5','subway_count_1']]\n",
    "apartment2 = pd.read_csv(\"Apartment_ga_hd.csv\")\n",
    "apartment2 = apartment2[['apartment_id','gangnam_dist']]\n",
    "apartment3 = pd.read_csv(\"apartment_school.csv\")\n",
    "apartment3 = apartment3[['apartment_id','elementary_0.5','middle_0.5','high_0.5','total_0.5']]\n",
    "apartment4 = pd.read_csv(\"min_distance_apartment.csv\")\n",
    "apartment4 = apartment4[['apartment_id','subway_min_distance','min_distance_ele','min_distance_middle','min_distance_high']]\n",
    "#apartment4 = pd.read_csv(\"apartment_bub.csv\")\n",
    "#apartment4 = apartment4[['apartment_id','gu','dong']]\n",
    "df_all = pd.merge(df_all,apartment1,on='apartment_id').reset_index(drop=True)\n",
    "df_all = pd.merge(df_all,apartment2,on='apartment_id').reset_index(drop=True)\n",
    "df_all = pd.merge(df_all,apartment3,on='apartment_id').reset_index(drop=True)\n",
    "df_all = pd.merge(df_all,apartment4,on='apartment_id').reset_index(drop=True).sort_values('index') #index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 공공데이터\n",
    "- 사용목록과 코드는 따로 첨부하였습니다. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 쇼핑몰\n",
    "busan_shop = pd.read_csv(\"(0119)busan_shop.csv\")\n",
    "busan_shop = busan_shop[['apartment_id','shop_count_0.5','shop_count_1']]\n",
    "df_all = pd.merge(df_all,busan_shop,on='apartment_id').reset_index(drop=True).sort_values('index') #index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 대학교\n",
    "university = pd.read_csv('apartment_public.csv')\n",
    "university = university[['apartment_id','univ_1,2']]\n",
    "df_all = pd.merge(df_all,university,on='apartment_id').reset_index(drop=True).sort_values('index') #index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 구청\n",
    "public = pd.read_csv('apartment_public.csv')\n",
    "public = public[['apartment_id','public_1']]\n",
    "public['public_1'] = public['public_1'].apply(lambda x: 1 if x>1 else x)\n",
    "df_all = pd.merge(df_all,public,on='apartment_id').reset_index(drop=True).sort_values('index') #index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 스타벅스\n",
    "coffee = pd.read_csv(\"(0122)starbucks_list.csv\")\n",
    "coffee = coffee[['apartment_id','shop_count_1']]\n",
    "coffee.columns = ['apartment_id','coffee_count_1']\n",
    "df_all = pd.merge(df_all,coffee,on='apartment_id').reset_index(drop=True)\n",
    "#df_all['distance_from_hangang_6.0'] = df_all['distance_from_hangang_6.0'].apply(lambda x: 1 if x >=1 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA, FastICA,NMF,LatentDirichletAllocation,IncrementalPCA,MiniBatchSparsePCA\n",
    "from sklearn.decomposition import TruncatedSVD,FactorAnalysis,KernelPCA\n",
    "\n",
    "train_df = df_all.loc[df_all['transaction_real_price'] != 0]\n",
    "test_df = df_all.loc[df_all['transaction_real_price'] == 0]\n",
    "\n",
    "train_len = train_df.shape[0]\n",
    "\n",
    "### 날짜와 object, 공공데이터는 제거. \n",
    "train_columns = [c for c in train_df.columns if c not in ['key','transaction_real_price','transaction_year_month_date','transaction_date','heat_type','heat_fuel',\n",
    "                                                          'front_door_structure','shop_count_0.5','shop_count_1','univ_1,2','public_1','coffee_count_0.5']]\n",
    "train_columns\n",
    "\n",
    "# PCA\n",
    "n_comp = 1\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=2019)\n",
    "ica2_results_train = ica.fit_transform(train_df[train_columns].fillna(-1))\n",
    "ica2_results_test = ica.transform(test_df[train_columns].fillna(-1))\n",
    "\n",
    "for i in range(1, n_comp+1):\n",
    "    train_df['ica_' + str(i)] = ica2_results_train[:,i-1]\n",
    "    test_df['ica_' + str(i)] = ica2_results_test[:, i-1]\n",
    "    train_columns.append('ica_' + str(i))\n",
    "\n",
    "df_all = pd.concat([train_df,test_df])\n",
    "df_all = df_all.sort_values('index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 날짜 형식변경\n",
    "df_all['transaction_date1'] = df_all.transaction_date.apply(lambda x: x[-2:])\n",
    "#convert int to date\n",
    "df_all['transaction_year_month1'] = df_all['transaction_year_month'].astype(str)\n",
    "#join month and date \n",
    "df_all['transaction_year_month_date'] = df_all[['transaction_year_month1', 'transaction_date1']].apply(lambda x: ''.join(x), axis=1)\n",
    "df_all['transaction_year_month_date'] = df_all['transaction_year_month_date'].astype(int)\n",
    "del df_all['transaction_date1']; del df_all['transaction_year_month1']\n",
    "del df_all['transaction_year_month']; del df_all['transaction_date'] ; del df_all['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One-hot-encoding\n",
    "df_all = pd.get_dummies(df_all)\n",
    "train = df_all[:train_len]\n",
    "df_test = df_all[train_len:]\n",
    "del df_all\n",
    "train = train.sort_values('index')\n",
    "df_test = df_test.sort_values('index')\n",
    "\n",
    "### 거리의 경우 너무 크면 의미가 없어서 상관관계를 통해서 파악한 16을 기준으로 잘라버림. \n",
    "train['gangnam_dist'] = train['gangnam_dist'].apply(lambda x: 16 if x > 16 else x)\n",
    "df_test['gangnam_dist'] = df_test['gangnam_dist'].apply(lambda x: 16 if x > 16 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIGHTGBM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_features = [\n",
    "    'transaction_real_price'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'apartment_id'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['index']\n",
    "del df_test['index']\n",
    "del train['mean_year_month']\n",
    "del df_test['mean_year_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_busan = train[train['city']==0].reset_index(drop=True)\n",
    "df_test_busan = df_test[df_test['city']==0].reset_index(drop=True)\n",
    "df_train_seoul = train[train['city']==1].reset_index(drop=True)\n",
    "df_test_seoul = df_test[df_test['city']==1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 부산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "y_train = df_train_busan['transaction_real_price'].reset_index(drop=True)\n",
    "\n",
    "x_train = df_train_busan.copy().reset_index(drop=True)\n",
    "del x_train['city']; del x_train['transaction_real_price']; del x_train['public_1']; \n",
    "\n",
    "\n",
    "excluded_features = ['key']\n",
    "train_features = [_f for _f in x_train.columns if _f not in excluded_features]\n",
    "\n",
    "busan_key = df_test_busan['key'].values\n",
    "x_test = df_test_busan[train_features].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 133 rounds.\n",
      "[1000]\ttraining's quantile: 0.0252562\tvalid_1's quantile: 0.0261465\n",
      "[2000]\ttraining's quantile: 0.023306\tvalid_1's quantile: 0.0245302\n",
      "[3000]\ttraining's quantile: 0.0223278\tvalid_1's quantile: 0.0238086\n",
      "[4000]\ttraining's quantile: 0.0216897\tvalid_1's quantile: 0.0233968\n",
      "[5000]\ttraining's quantile: 0.0211853\tvalid_1's quantile: 0.0231146\n",
      "[6000]\ttraining's quantile: 0.0207909\tvalid_1's quantile: 0.0229068\n",
      "[7000]\ttraining's quantile: 0.0204402\tvalid_1's quantile: 0.0227712\n",
      "[8000]\ttraining's quantile: 0.0201422\tvalid_1's quantile: 0.0226599\n",
      "[9000]\ttraining's quantile: 0.0198751\tvalid_1's quantile: 0.0225564\n",
      "[10000]\ttraining's quantile: 0.0196367\tvalid_1's quantile: 0.0224772\n",
      "[11000]\ttraining's quantile: 0.0194188\tvalid_1's quantile: 0.0224186\n",
      "[12000]\ttraining's quantile: 0.0192115\tvalid_1's quantile: 0.0223617\n",
      "[13000]\ttraining's quantile: 0.0190178\tvalid_1's quantile: 0.022321\n",
      "[14000]\ttraining's quantile: 0.018836\tvalid_1's quantile: 0.022285\n",
      "[15000]\ttraining's quantile: 0.0186638\tvalid_1's quantile: 0.0222456\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's quantile: 0.0186638\tvalid_1's quantile: 0.0222456\n",
      "Training until validation scores don't improve for 133 rounds.\n",
      "[1000]\ttraining's quantile: 0.0252441\tvalid_1's quantile: 0.0259528\n",
      "[2000]\ttraining's quantile: 0.0232798\tvalid_1's quantile: 0.0242906\n",
      "[3000]\ttraining's quantile: 0.0223211\tvalid_1's quantile: 0.0235878\n",
      "[4000]\ttraining's quantile: 0.02167\tvalid_1's quantile: 0.0231761\n",
      "[5000]\ttraining's quantile: 0.0211792\tvalid_1's quantile: 0.0229069\n",
      "[6000]\ttraining's quantile: 0.0207697\tvalid_1's quantile: 0.0227107\n",
      "[7000]\ttraining's quantile: 0.0204191\tvalid_1's quantile: 0.0225544\n",
      "[8000]\ttraining's quantile: 0.0201287\tvalid_1's quantile: 0.0224353\n",
      "[9000]\ttraining's quantile: 0.0198568\tvalid_1's quantile: 0.0223442\n",
      "[10000]\ttraining's quantile: 0.0196115\tvalid_1's quantile: 0.0222613\n",
      "[11000]\ttraining's quantile: 0.0193881\tvalid_1's quantile: 0.0221961\n",
      "[12000]\ttraining's quantile: 0.019177\tvalid_1's quantile: 0.0221422\n",
      "[13000]\ttraining's quantile: 0.0189929\tvalid_1's quantile: 0.022098\n",
      "[14000]\ttraining's quantile: 0.0188143\tvalid_1's quantile: 0.022064\n",
      "[15000]\ttraining's quantile: 0.018648\tvalid_1's quantile: 0.0220323\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's quantile: 0.018648\tvalid_1's quantile: 0.0220323\n",
      "Training until validation scores don't improve for 133 rounds.\n",
      "[1000]\ttraining's quantile: 0.025261\tvalid_1's quantile: 0.0259126\n",
      "[2000]\ttraining's quantile: 0.023273\tvalid_1's quantile: 0.0242515\n",
      "[3000]\ttraining's quantile: 0.0223079\tvalid_1's quantile: 0.023555\n",
      "[4000]\ttraining's quantile: 0.0216513\tvalid_1's quantile: 0.0231444\n",
      "[5000]\ttraining's quantile: 0.0211652\tvalid_1's quantile: 0.0228883\n",
      "[6000]\ttraining's quantile: 0.0207784\tvalid_1's quantile: 0.0227023\n",
      "[7000]\ttraining's quantile: 0.0204251\tvalid_1's quantile: 0.0225481\n",
      "[8000]\ttraining's quantile: 0.0201276\tvalid_1's quantile: 0.0224415\n",
      "[9000]\ttraining's quantile: 0.0198625\tvalid_1's quantile: 0.0223487\n",
      "[10000]\ttraining's quantile: 0.0196166\tvalid_1's quantile: 0.0222744\n",
      "[11000]\ttraining's quantile: 0.0193907\tvalid_1's quantile: 0.0222062\n",
      "[12000]\ttraining's quantile: 0.0191837\tvalid_1's quantile: 0.022158\n",
      "[13000]\ttraining's quantile: 0.0189945\tvalid_1's quantile: 0.0221072\n",
      "[14000]\ttraining's quantile: 0.0188112\tvalid_1's quantile: 0.0220698\n",
      "[15000]\ttraining's quantile: 0.0186405\tvalid_1's quantile: 0.0220384\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's quantile: 0.0186405\tvalid_1's quantile: 0.0220384\n",
      "Training until validation scores don't improve for 133 rounds.\n",
      "[1000]\ttraining's quantile: 0.0252878\tvalid_1's quantile: 0.0259662\n",
      "[2000]\ttraining's quantile: 0.0233013\tvalid_1's quantile: 0.024313\n",
      "[3000]\ttraining's quantile: 0.0223149\tvalid_1's quantile: 0.0236057\n",
      "[4000]\ttraining's quantile: 0.0216551\tvalid_1's quantile: 0.0231992\n",
      "[5000]\ttraining's quantile: 0.0211596\tvalid_1's quantile: 0.0229259\n",
      "[6000]\ttraining's quantile: 0.0207553\tvalid_1's quantile: 0.0227248\n",
      "[7000]\ttraining's quantile: 0.0204188\tvalid_1's quantile: 0.0225786\n",
      "[8000]\ttraining's quantile: 0.0201287\tvalid_1's quantile: 0.022462\n",
      "[9000]\ttraining's quantile: 0.019869\tvalid_1's quantile: 0.0223807\n",
      "[10000]\ttraining's quantile: 0.0196301\tvalid_1's quantile: 0.0223084\n",
      "[11000]\ttraining's quantile: 0.0194064\tvalid_1's quantile: 0.0222457\n",
      "[12000]\ttraining's quantile: 0.0192039\tvalid_1's quantile: 0.0221946\n",
      "[13000]\ttraining's quantile: 0.0190141\tvalid_1's quantile: 0.0221459\n",
      "[14000]\ttraining's quantile: 0.0188376\tvalid_1's quantile: 0.0221048\n",
      "[15000]\ttraining's quantile: 0.0186695\tvalid_1's quantile: 0.0220707\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's quantile: 0.0186695\tvalid_1's quantile: 0.0220707\n",
      "Training until validation scores don't improve for 133 rounds.\n",
      "[1000]\ttraining's quantile: 0.025283\tvalid_1's quantile: 0.0257472\n",
      "[2000]\ttraining's quantile: 0.0232929\tvalid_1's quantile: 0.0240996\n",
      "[3000]\ttraining's quantile: 0.022309\tvalid_1's quantile: 0.0233897\n",
      "[4000]\ttraining's quantile: 0.0216585\tvalid_1's quantile: 0.0229763\n",
      "[5000]\ttraining's quantile: 0.0211587\tvalid_1's quantile: 0.0227172\n",
      "[6000]\ttraining's quantile: 0.0207564\tvalid_1's quantile: 0.022521\n",
      "[7000]\ttraining's quantile: 0.0204157\tvalid_1's quantile: 0.022374\n",
      "[8000]\ttraining's quantile: 0.020122\tvalid_1's quantile: 0.0222682\n",
      "[9000]\ttraining's quantile: 0.0198567\tvalid_1's quantile: 0.0221713\n",
      "[10000]\ttraining's quantile: 0.0196166\tvalid_1's quantile: 0.0220984\n",
      "[11000]\ttraining's quantile: 0.0193969\tvalid_1's quantile: 0.0220372\n",
      "[12000]\ttraining's quantile: 0.0191937\tvalid_1's quantile: 0.0220003\n",
      "[13000]\ttraining's quantile: 0.0190002\tvalid_1's quantile: 0.0219489\n",
      "[14000]\ttraining's quantile: 0.0188207\tvalid_1's quantile: 0.021919\n",
      "[15000]\ttraining's quantile: 0.0186591\tvalid_1's quantile: 0.0218882\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's quantile: 0.0186591\tvalid_1's quantile: 0.0218882\n",
      "Full rmse score 17670587.718873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "folds = KFold(n_splits=5,random_state=6,shuffle=True)\n",
    "oof_preds = np.zeros(x_train.shape[0])\n",
    "sub_preds = np.zeros(x_test.shape[0])\n",
    "\n",
    "start = time.time()\n",
    "valid_score = 0\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(x_train, y_train)):\n",
    "    trn_x, trn_y = x_train[train_features].iloc[trn_idx], y_train[trn_idx]\n",
    "    val_x, val_y = x_train[train_features].iloc[val_idx], y_train[val_idx]  \n",
    "    \n",
    "    train_data = lgb.Dataset(data=trn_x, label=trn_y)\n",
    "    valid_data = lgb.Dataset(data=val_x, label=val_y)   \n",
    "    \n",
    "    params = {\"objective\" : \"regression\", \"metric\" : \"quantile\", 'n_estimators':15000, 'early_stopping_rounds':133,\n",
    "              \"num_leaves\" : 20, \"learning_rate\" : 0.18, \"bagging_fraction\" : 0.8,\n",
    "               \"bagging_seed\" : 0, 'min_data_in_leaf': 1144, 'max_depth': 6}\n",
    "    \n",
    "    lgb_model = lgb.train(params, train_data, valid_sets=[train_data, valid_data], verbose_eval=1000) \n",
    "    \n",
    "    oof_preds[val_idx] = lgb_model.predict(val_x, num_iteration=lgb_model.best_iteration)\n",
    "    sub_pred = lgb_model.predict(x_test, num_iteration=lgb_model.best_iteration) / folds.n_splits\n",
    "    sub_preds += sub_pred\n",
    "    \n",
    "    #print('Fold %2d rmse : %.6f' % (n_fold + 1, np.sqrt(mean_squared_error(val_y, oof_preds[val_idx]))))\n",
    "    valid_score += mean_squared_error(val_y, oof_preds[val_idx])\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = train_features\n",
    "    fold_importance_df[\"importance\"] = lgb_model.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = n_fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "print('Full rmse score %.6f\\n' % np.sqrt(mean_squared_error(np.expm1(y_train), np.expm1(oof_preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['transaction_real_price'] = oof_preds\n",
    "x_train.to_csv(\"Lightgbm_Bestmodel_busan_train_not_quantile.csv\",index=False)\n",
    "\n",
    "x_test['transaction_real_price'] = sub_preds\n",
    "x_test['key'] = busan_key\n",
    "x_test.to_csv(\"Lightgbm_Bestmodel_busan_test_not_quantile.csv\",index=False)\n",
    "\n",
    "sub_busan_not = x_test[['key','transaction_real_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full rmse score 17321852.956278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "busan_valid.columns = ['key','valid_price']\n",
    "busan_valid = pd.merge(busan_valid,x_train,on='key',how='left')\n",
    "\n",
    "\n",
    "print('Full rmse score %.6f\\n' % np.sqrt(mean_squared_error(np.expm1(busan_valid['transaction_real_price']), busan_valid['valid_price'])))\n",
    "busan_score_not = np.sqrt(mean_squared_error(np.expm1(busan_valid['transaction_real_price']), busan_valid['valid_price']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 서울"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "y_train = df_train_seoul['transaction_real_price'].reset_index(drop=True)\n",
    "\n",
    "x_train = df_train_seoul.copy().reset_index(drop=True)\n",
    "del x_train['city']; del x_train['transaction_real_price']; del x_train['last_price_3'];  del x_train['ica_1']\n",
    "\n",
    "\n",
    "excluded_features = ['key','floor']\n",
    "train_features = [_f for _f in x_train.columns if _f not in excluded_features]\n",
    "\n",
    "seoul_key = df_test_seoul['key'].values\n",
    "seoul_floor = df_test_seoul['floor'].values\n",
    "x_test = df_test_seoul[train_features].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 110 rounds.\n",
      "[1000]\ttraining's quantile: 0.0211444\tvalid_1's quantile: 0.0219802\n",
      "[2000]\ttraining's quantile: 0.01884\tvalid_1's quantile: 0.0201597\n",
      "[3000]\ttraining's quantile: 0.0176141\tvalid_1's quantile: 0.0193894\n",
      "[4000]\ttraining's quantile: 0.0167484\tvalid_1's quantile: 0.0189333\n",
      "[5000]\ttraining's quantile: 0.016072\tvalid_1's quantile: 0.0186402\n",
      "[6000]\ttraining's quantile: 0.0155061\tvalid_1's quantile: 0.018429\n",
      "[7000]\ttraining's quantile: 0.0150164\tvalid_1's quantile: 0.0182784\n",
      "[8000]\ttraining's quantile: 0.0145803\tvalid_1's quantile: 0.0181465\n",
      "[9000]\ttraining's quantile: 0.0141887\tvalid_1's quantile: 0.0180498\n",
      "[10000]\ttraining's quantile: 0.0138241\tvalid_1's quantile: 0.017966\n",
      "[11000]\ttraining's quantile: 0.0134902\tvalid_1's quantile: 0.0179078\n",
      "[12000]\ttraining's quantile: 0.0131789\tvalid_1's quantile: 0.0178531\n",
      "[13000]\ttraining's quantile: 0.0128864\tvalid_1's quantile: 0.0178033\n",
      "[14000]\ttraining's quantile: 0.0126091\tvalid_1's quantile: 0.0177647\n",
      "[15000]\ttraining's quantile: 0.0123439\tvalid_1's quantile: 0.0177346\n",
      "[16000]\ttraining's quantile: 0.0121017\tvalid_1's quantile: 0.0177021\n",
      "[17000]\ttraining's quantile: 0.0118614\tvalid_1's quantile: 0.0176774\n",
      "[18000]\ttraining's quantile: 0.011633\tvalid_1's quantile: 0.0176548\n",
      "Early stopping, best iteration is:\n",
      "[18336]\ttraining's quantile: 0.0115602\tvalid_1's quantile: 0.0176467\n",
      "Training until validation scores don't improve for 110 rounds.\n",
      "[1000]\ttraining's quantile: 0.0211877\tvalid_1's quantile: 0.0218577\n",
      "[2000]\ttraining's quantile: 0.0188516\tvalid_1's quantile: 0.0200449\n",
      "[3000]\ttraining's quantile: 0.0176045\tvalid_1's quantile: 0.0192439\n",
      "[4000]\ttraining's quantile: 0.0167498\tvalid_1's quantile: 0.0188099\n",
      "[5000]\ttraining's quantile: 0.0160796\tvalid_1's quantile: 0.0185281\n",
      "[6000]\ttraining's quantile: 0.0155152\tvalid_1's quantile: 0.0183223\n",
      "[7000]\ttraining's quantile: 0.0150168\tvalid_1's quantile: 0.0181604\n",
      "[8000]\ttraining's quantile: 0.014577\tvalid_1's quantile: 0.0180342\n",
      "[9000]\ttraining's quantile: 0.0141822\tvalid_1's quantile: 0.0179296\n",
      "[10000]\ttraining's quantile: 0.0138205\tvalid_1's quantile: 0.0178525\n",
      "[11000]\ttraining's quantile: 0.0134883\tvalid_1's quantile: 0.0177837\n",
      "[12000]\ttraining's quantile: 0.0131775\tvalid_1's quantile: 0.0177309\n",
      "[13000]\ttraining's quantile: 0.0128821\tvalid_1's quantile: 0.0176825\n",
      "[14000]\ttraining's quantile: 0.0126086\tvalid_1's quantile: 0.0176515\n",
      "[15000]\ttraining's quantile: 0.0123434\tvalid_1's quantile: 0.0176238\n",
      "[16000]\ttraining's quantile: 0.0120919\tvalid_1's quantile: 0.0175919\n",
      "[17000]\ttraining's quantile: 0.0118597\tvalid_1's quantile: 0.0175679\n",
      "[18000]\ttraining's quantile: 0.0116387\tvalid_1's quantile: 0.0175497\n",
      "[19000]\ttraining's quantile: 0.0114211\tvalid_1's quantile: 0.0175331\n",
      "[20000]\ttraining's quantile: 0.0112117\tvalid_1's quantile: 0.0175142\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20000]\ttraining's quantile: 0.0112117\tvalid_1's quantile: 0.0175142\n",
      "Training until validation scores don't improve for 110 rounds.\n",
      "[1000]\ttraining's quantile: 0.021151\tvalid_1's quantile: 0.0219689\n",
      "[2000]\ttraining's quantile: 0.0188198\tvalid_1's quantile: 0.0201529\n",
      "[3000]\ttraining's quantile: 0.0176015\tvalid_1's quantile: 0.0193853\n",
      "[4000]\ttraining's quantile: 0.0167374\tvalid_1's quantile: 0.0189254\n",
      "[5000]\ttraining's quantile: 0.0160757\tvalid_1's quantile: 0.0186518\n",
      "[6000]\ttraining's quantile: 0.0155153\tvalid_1's quantile: 0.0184474\n",
      "[7000]\ttraining's quantile: 0.0150278\tvalid_1's quantile: 0.018289\n",
      "[8000]\ttraining's quantile: 0.014595\tvalid_1's quantile: 0.018163\n",
      "[9000]\ttraining's quantile: 0.0141955\tvalid_1's quantile: 0.0180478\n",
      "[10000]\ttraining's quantile: 0.0138411\tvalid_1's quantile: 0.0179661\n",
      "[11000]\ttraining's quantile: 0.0135027\tvalid_1's quantile: 0.017894\n",
      "[12000]\ttraining's quantile: 0.0131934\tvalid_1's quantile: 0.0178332\n",
      "[13000]\ttraining's quantile: 0.0128935\tvalid_1's quantile: 0.0177771\n",
      "Early stopping, best iteration is:\n",
      "[13467]\ttraining's quantile: 0.0127611\tvalid_1's quantile: 0.0177563\n",
      "Training until validation scores don't improve for 110 rounds.\n",
      "[1000]\ttraining's quantile: 0.0211261\tvalid_1's quantile: 0.0218484\n",
      "[2000]\ttraining's quantile: 0.0188126\tvalid_1's quantile: 0.0200692\n",
      "[3000]\ttraining's quantile: 0.0175996\tvalid_1's quantile: 0.0193381\n",
      "[4000]\ttraining's quantile: 0.0167406\tvalid_1's quantile: 0.0189023\n",
      "[5000]\ttraining's quantile: 0.0160703\tvalid_1's quantile: 0.0186237\n",
      "[6000]\ttraining's quantile: 0.0155032\tvalid_1's quantile: 0.0184108\n",
      "[7000]\ttraining's quantile: 0.0150144\tvalid_1's quantile: 0.0182536\n",
      "[8000]\ttraining's quantile: 0.0145732\tvalid_1's quantile: 0.0181329\n",
      "[9000]\ttraining's quantile: 0.0141797\tvalid_1's quantile: 0.0180391\n",
      "[10000]\ttraining's quantile: 0.0138155\tvalid_1's quantile: 0.0179555\n",
      "[11000]\ttraining's quantile: 0.0134846\tvalid_1's quantile: 0.0178969\n",
      "[12000]\ttraining's quantile: 0.0131683\tvalid_1's quantile: 0.0178415\n",
      "[13000]\ttraining's quantile: 0.0128711\tvalid_1's quantile: 0.0178013\n",
      "[14000]\ttraining's quantile: 0.0125966\tvalid_1's quantile: 0.0177597\n",
      "[15000]\ttraining's quantile: 0.0123391\tvalid_1's quantile: 0.0177235\n",
      "Early stopping, best iteration is:\n",
      "[15693]\ttraining's quantile: 0.0121665\tvalid_1's quantile: 0.0176997\n",
      "Training until validation scores don't improve for 110 rounds.\n",
      "[1000]\ttraining's quantile: 0.0211993\tvalid_1's quantile: 0.0219365\n",
      "[2000]\ttraining's quantile: 0.0189008\tvalid_1's quantile: 0.0201285\n",
      "[3000]\ttraining's quantile: 0.0176647\tvalid_1's quantile: 0.01933\n",
      "[4000]\ttraining's quantile: 0.0168197\tvalid_1's quantile: 0.0189054\n",
      "[5000]\ttraining's quantile: 0.0161359\tvalid_1's quantile: 0.018599\n",
      "[6000]\ttraining's quantile: 0.0155709\tvalid_1's quantile: 0.0183858\n",
      "[7000]\ttraining's quantile: 0.0150811\tvalid_1's quantile: 0.0182227\n",
      "[8000]\ttraining's quantile: 0.0146505\tvalid_1's quantile: 0.0181068\n",
      "[9000]\ttraining's quantile: 0.014254\tvalid_1's quantile: 0.0180125\n",
      "[10000]\ttraining's quantile: 0.013891\tvalid_1's quantile: 0.0179324\n",
      "[11000]\ttraining's quantile: 0.0135532\tvalid_1's quantile: 0.0178683\n",
      "[12000]\ttraining's quantile: 0.0132411\tvalid_1's quantile: 0.0178066\n",
      "[13000]\ttraining's quantile: 0.01295\tvalid_1's quantile: 0.0177623\n",
      "[14000]\ttraining's quantile: 0.0126742\tvalid_1's quantile: 0.017723\n",
      "[15000]\ttraining's quantile: 0.0124141\tvalid_1's quantile: 0.017688\n",
      "Early stopping, best iteration is:\n",
      "[15653]\ttraining's quantile: 0.0122539\tvalid_1's quantile: 0.0176676\n",
      "Full rmse score 36044003.950589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "import time\n",
    "folds = KFold(n_splits=5,random_state=6,shuffle=True)\n",
    "oof_preds = np.zeros(x_train.shape[0])\n",
    "sub_preds = np.zeros(x_test.shape[0])\n",
    "\n",
    "start = time.time()\n",
    "valid_score = 0\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(x_train, y_train)):\n",
    "    trn_x, trn_y = x_train[train_features].iloc[trn_idx], y_train[trn_idx]\n",
    "    val_x, val_y = x_train[train_features].iloc[val_idx], y_train[val_idx]  \n",
    "    \n",
    "    train_data = lgb.Dataset(data=trn_x, label=trn_y)\n",
    "    valid_data = lgb.Dataset(data=val_x, label=val_y)   \n",
    "    \n",
    "    params = {\"objective\" : \"regression\", \"metric\" : \"quantile\", 'n_estimators': 20000, 'early_stopping_rounds':110,\n",
    "              \"num_leaves\" : 30, \"learning_rate\" : 0.15, \"bagging_fraction\" : 0.9, \"lambda_l1\" : 0.1,\n",
    "               \"bagging_seed\" : 0}\n",
    "    \n",
    "    lgb_model = lgb.train(params, train_data, valid_sets=[train_data, valid_data], verbose_eval=1000) \n",
    "    \n",
    "    oof_preds[val_idx] = lgb_model.predict(val_x, num_iteration=lgb_model.best_iteration)\n",
    "    sub_pred = lgb_model.predict(x_test, num_iteration=lgb_model.best_iteration) / folds.n_splits\n",
    "    sub_preds += sub_pred\n",
    "    \n",
    "    #print('Fold %2d rmse : %.6f' % (n_fold + 1, np.sqrt(mean_squared_error(val_y, oof_preds[val_idx]))))\n",
    "    valid_score += mean_squared_error(val_y, oof_preds[val_idx])\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = train_features\n",
    "    fold_importance_df[\"importance\"] = lgb_model.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = n_fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "print('Full rmse score %.6f\\n' % np.sqrt(mean_squared_error(np.expm1(y_train), np.expm1(oof_preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['transaction_real_price'] = oof_preds\n",
    "x_train.to_csv(\"Lightgbm_Bestmodel_seoul_train_not_quantile.csv\",index=False)\n",
    "\n",
    "x_test['transaction_real_price'] = sub_preds\n",
    "x_test['key'] = seoul_key\n",
    "x_test['floor'] = seoul_floor\n",
    "\n",
    "x_test.to_csv(\"Lightgbm_Bestmodel_seoul_test_not_quantile.csv\",index=False)\n",
    "\n",
    "sub_seoul_not = x_test[['key','transaction_real_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full rmse score 34939711.577105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seoul_valid.columns = ['key','valid_price']\n",
    "seoul_valid = pd.merge(seoul_valid,x_train,on='key',how='left')\n",
    "\n",
    "\n",
    "print('Full rmse score %.6f\\n' % np.sqrt(mean_squared_error(np.expm1(seoul_valid['transaction_real_price']), seoul_valid['valid_price'])))\n",
    "busan_score_not = np.sqrt(mean_squared_error(np.expm1(seoul_valid['transaction_real_price']), seoul_valid['valid_price']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>transaction_real_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1253422</td>\n",
       "      <td>18.537149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1369751</td>\n",
       "      <td>18.795176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1389544</td>\n",
       "      <td>18.680887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1394472</td>\n",
       "      <td>19.820289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1395869</td>\n",
       "      <td>19.367000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       key  transaction_real_price\n",
       "0  1253422               18.537149\n",
       "1  1369751               18.795176\n",
       "2  1389544               18.680887\n",
       "3  1394472               19.820289\n",
       "4  1395869               19.367000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_not_deep = pd.concat([sub_busan_not,sub_seoul_not])\n",
    "sub_not_deep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_not_deep['transaction_real_price'] = np.expm1(sub_not_deep['transaction_real_price'])\n",
    "sub_not_deep = sub_not_deep.sort_values('key')\n",
    "sub_not_deep = sub_not_deep.reset_index(drop=True)\n",
    "sub_not_deep.to_csv(\"[190130]LGB_Quantile_not_deep.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
