{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Analysis \n",
    "import matplotlib.pyplot as plt #Visulization\n",
    "import seaborn as sns #Visulization\n",
    "import numpy as np #Analysis \n",
    "from scipy.stats import norm #Analysis \n",
    "from sklearn.preprocessing import StandardScaler #Analysis \n",
    "from scipy import stats #Analysis \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test_idx = test['key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리 준비작업\n",
    "- 인천 수정\n",
    "- Validation 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인천의 경우 서울보다는 부산에 가까워서 city를 부산으로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test['key']==1503614,'city'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 시간의 순서대로 이루어져 있어서 Merge과정에서 순서가 깨지지 않도록 index컬럼을 생성해서 sort작업을 진행해줄것임. \n",
    "- 이를 안해주면 fold에서 다른 cv값이 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_len = train.shape[0]\n",
    "df_all = pd.concat([train,test])\n",
    "\n",
    "index = []\n",
    "for i in range(0,df_all.shape[0]):\n",
    "    index.append(i)\n",
    "df_all['index'] = index\n",
    "\n",
    "train = df_all[:train_len].reset_index(drop=True)\n",
    "df_test = df_all[train_len:].reset_index(drop=True)\n",
    "\n",
    "df_train_busan = train[train['city']==0]\n",
    "df_test_busan = df_test[df_test['city']==0]\n",
    "df_train_seoul = train[train['city']==1]\n",
    "df_test_seoul = df_test[df_test['city']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신뢰성 있는 Validation 구축을 위해서 아파트 별로 가장 마지막 거래를 Validation으로 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "busan_val_idx = df_train_busan.groupby(['apartment_id']).tail(1).index\n",
    "busan_valid = df_train_busan.loc[busan_val_idx,:]\n",
    "busan_valid = busan_valid[['key','transaction_real_price']]\n",
    "\n",
    "seoul_val_idx = df_train_seoul.groupby(['apartment_id']).tail(1).index\n",
    "seoul_valid = df_train_seoul.loc[seoul_val_idx,:]\n",
    "seoul_valid = seoul_valid[['key','transaction_real_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_train_busan,df_train_seoul,df_test_busan,df_test_seoul\n",
    "del df_test,train,df_all\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "- 트레인은 날짜가 고른 반면, 테스트는 2018년도 6월 이후가 압도적으로 많음. 샘플링 작업이 필요.\n",
    "- 방과 화장실 0인 값 대체 : 동일한 아파트에서 비슷한 크기에 값이 존재하면 그로 채워넣고, 그렇지 않으면 비슷한 크기에서 median으로 채워 넣음. \n",
    "- 방과 화장실 결측치 대체 : 동일한 아파트에서 비슷한 크기에 값이 존재하면 그로 채워넣고, 그렇지 않으면 비슷한 크기에서 median으로 채워 넣음. \n",
    "- 주차장의 결측치는 0으로 대체\n",
    "- 난방과 현관구조는 None으로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test_idx = test['key']\n",
    "test.loc[test['key']==1503614,'city'] = 0\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "gc.collect()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_len = train.shape[0]\n",
    "df_all = pd.concat([train,test])\n",
    "\n",
    "index = []\n",
    "for i in range(0,df_all.shape[0]):\n",
    "    index.append(i)\n",
    "df_all['index'] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 0인 값 대체\n",
    "df_all.loc[(df_all['apartment_id']==2805) & (df_all['supply_area'] > 90),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==2805) & (df_all['supply_area'] > 90),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==895) & (df_all['supply_area'] > 137),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==895) & (df_all['supply_area'] > 137),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==903) & (df_all['supply_area'] > 135),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==903) & (df_all['supply_area'] > 135),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==1622) & (df_all['supply_area'] == 127.07),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==1622) & (df_all['supply_area'] == 127.07),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==14029) & (df_all['supply_area'] > 100),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==14029) & (df_all['supply_area'] > 100),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==14029) & (df_all['supply_area'] < 100),'room_count'] = 2\n",
    "df_all.loc[(df_all['apartment_id']==14029) & (df_all['supply_area'] < 100),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==12067) & (df_all['supply_area'] >= 95),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==12067) & (df_all['supply_area'] >= 95),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==12067) & (df_all['supply_area'] == 92),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==12067) & (df_all['supply_area'] == 92),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==12067) & (df_all['supply_area'] < 90),'room_count'] = 2\n",
    "df_all.loc[(df_all['apartment_id']==12067) & (df_all['supply_area'] < 90),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==360) & (df_all['supply_area'] == 189.99),'room_count'] = 5\n",
    "df_all.loc[(df_all['apartment_id']==360) & (df_all['supply_area'] == 189.99),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==360) & (df_all['supply_area'] == 154.46),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==360) & (df_all['supply_area'] == 154.46),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==568) & (df_all['supply_area']//10 == 11.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==568) & (df_all['supply_area']//10 == 11.0),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==1543) & (df_all['supply_area'] > 150 ),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==1543) & (df_all['supply_area'] > 150),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==618) & (df_all['supply_area'] == 92.94),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==618) & (df_all['supply_area'] == 92.94),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==618) & (df_all['supply_area'] == 110.57),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==618) & (df_all['supply_area'] == 110.57),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==4368) & (df_all['supply_area'] > 90),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==4368) & (df_all['supply_area'] > 90),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==4368) & (df_all['supply_area'] < 90),'room_count'] = 2\n",
    "df_all.loc[(df_all['apartment_id']==4368) & (df_all['supply_area'] < 90),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==3701) & (df_all['supply_area'] == 148.55),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==3701) & (df_all['supply_area'] == 148.55),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==346) & (df_all['supply_area'] > 100),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==346) & (df_all['supply_area'] > 100),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==1524) & (df_all['supply_area'] == 104.39),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==1524) & (df_all['supply_area'] == 104.39),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==1524) & (df_all['supply_area'] == 175.60),'room_count'] = 5\n",
    "df_all.loc[(df_all['apartment_id']==1524) & (df_all['supply_area'] == 175.60),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 9.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 9.0),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 10.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 10.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 13.0),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 13.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 14.0),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area']//10 == 14.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==1112) & (df_all['supply_area']//10 == 7.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==1112) & (df_all['supply_area']//10 == 7.0),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==65) & (df_all['supply_area']//10 == 11.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==65) & (df_all['supply_area']//10 == 11.0),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==541) & (df_all['supply_area']//10 == 8.0),'room_count'] = 2\n",
    "df_all.loc[(df_all['apartment_id']==541) & (df_all['supply_area']//10 == 8.0),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==184) & (df_all['supply_area'] == 66.12),'room_count'] = 2\n",
    "df_all.loc[(df_all['apartment_id']==184) & (df_all['supply_area'] == 66.12),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==2601) & (df_all['supply_area'] == 104.97),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==2601) & (df_all['supply_area'] == 104.97),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==6161) & (df_all['supply_area'] == 99.91),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==6161) & (df_all['supply_area'] == 99.91),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==3685) & (df_all['supply_area'] == 115.70),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==3685) & (df_all['supply_area'] == 115.70),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==21288) & (df_all['supply_area'] == 116.03),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==21288) & (df_all['supply_area'] == 116.03),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==10636) & (df_all['supply_area'] == 112.40),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==10636) & (df_all['supply_area'] == 112.40),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==1162) & (df_all['supply_area'] == 154.71),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==1162) & (df_all['supply_area'] == 154.71),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==10989) & (df_all['supply_area'] == 110.51),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==10989) & (df_all['supply_area'] == 110.51),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==11096) & (df_all['supply_area'] == 97.09),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==11096) & (df_all['supply_area'] == 97.09),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==184) & (df_all['supply_area'] == 69.42),'room_count'] = 2\n",
    "df_all.loc[(df_all['apartment_id']==184) & (df_all['supply_area'] == 69.42),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==534) & (df_all['supply_area'] //10 == 11.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==534) & (df_all['supply_area'] //10 == 11.0),'bathroom_count'] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==17384) & (df_all['supply_area'] //10 == 11.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==17384) & (df_all['supply_area'] //10 == 11.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area'] //10 == 10.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==431) & (df_all['supply_area'] //10 == 10.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==4058) & (df_all['supply_area'] //10 == 9.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==4058) & (df_all['supply_area'] //10 == 9.0),'bathroom_count'] = 2\n",
    "\n",
    "#df_all.loc[(df_all['apartment_id']==1388) & (df_all['room_count']==0)]\n",
    "df_all.loc[(df_all['apartment_id']==1388) & (df_all['supply_area'] //10 == 14.0),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==1388) & (df_all['supply_area'] //10 == 14.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==7136) & (df_all['supply_area'] //10 == 7.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==7136) & (df_all['supply_area'] //10 == 7.0),'bathroom_count'] = 1\n",
    "\n",
    "#df_all.loc[(df_all['apartment_id']==18737)]\n",
    "df_all.loc[(df_all['apartment_id']==18737) & (df_all['supply_area'] //10 == 17.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==18737) & (df_all['supply_area'] //10 == 17.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==18741) & (df_all['supply_area'] >= 160),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==18741) & (df_all['supply_area'] >= 160),'bathroom_count'] = 2\n",
    "\n",
    "#df_all.loc[(df_all['apartment_id']==18732)]\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==18732) & (df_all['supply_area'] //10 == 11.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==18732) & (df_all['supply_area'] //10 == 11.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==18732) & (df_all['supply_area'] //10 == 18.0),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==18732) & (df_all['supply_area'] //10 == 18.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==360) & (df_all['supply_area'] //10 == 16.0),'room_count'] = 4\n",
    "df_all.loc[(df_all['apartment_id']==360) & (df_all['supply_area'] //10 == 16.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==8460) & (df_all['supply_area'] //10 == 8.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==8460) & (df_all['supply_area'] //10 == 8.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==6175) & (df_all['supply_area'] > 290),'room_count'] = 5\n",
    "df_all.loc[(df_all['apartment_id']==6175) & (df_all['supply_area'] > 290),'bathroom_count'] = 3\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==6232)]\n",
    "df_all.loc[(df_all['apartment_id']==6232) & (df_all['supply_area'] //10 == 19.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==6232) & (df_all['supply_area'] //10 == 19.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==15502) & (df_all['supply_area'] //10 == 10.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==15502) & (df_all['supply_area'] //10 == 10.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==568) & (df_all['supply_area'] //10 == 19.0),'room_count'] = 5\n",
    "df_all.loc[(df_all['apartment_id']==568) & (df_all['supply_area'] //10 == 19.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==16837) & (df_all['supply_area'] //10 == 9.0),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==16837) & (df_all['supply_area'] //10 == 9.0),'bathroom_count'] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id']==37468) & (df_all['supply_area']  <= 200),'room_count'] = 3\n",
    "df_all.loc[(df_all['apartment_id']==37468) & (df_all['supply_area']  <= 200),'bathroom_count'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결측치 대체\n",
    "- 방, 화장실은 위와 동일한 방식으로 채워넣음.\n",
    "- 주차장의 경우 0으로 대체. dacon에 물어본 결과 결측치는 0이라고 했음.\n",
    "- 히트 및 현관의 결측치는 None으로 대체. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 방과 화장실 결측치\n",
    "df_all.loc[df_all['apartment_id'] == 9005, ['room_count']] = 1\n",
    "df_all.loc[df_all['apartment_id'] == 9005, ['bathroom_count']] = 1\n",
    "\n",
    "df_all.loc[df_all['apartment_id'] == 1179, ['room_count']] = 4\n",
    "df_all.loc[df_all['apartment_id'] == 1179, ['bathroom_count']] = 2\n",
    "\n",
    "df_all.loc[df_all['apartment_id'] == 10627, ['room_count']] = 3\n",
    "df_all.loc[df_all['apartment_id'] == 10627, ['bathroom_count']] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id'] == 10627) & (df_all['supply_area'] == 56.61), ['room_count']] = 2\n",
    "df_all.loc[(df_all['apartment_id'] == 10627) & (df_all['supply_area'] == 56.61), ['bathroom_count']] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id'] == 7992) , ['room_count']] = 3\n",
    "df_all.loc[(df_all['apartment_id'] == 7992) & (df_all['supply_area'] <= 81), ['bathroom_count']] = 1\n",
    "df_all.loc[(df_all['apartment_id'] == 7992) & (df_all['supply_area'] > 81), ['bathroom_count']] = 2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id'] == 7118) & (df_all['supply_area'] == 75.55), ['room_count']] = 3\n",
    "df_all.loc[(df_all['apartment_id'] == 7118) & (df_all['supply_area'] == 75.55), ['bathroom_count']] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id'] == 7118) & (df_all['supply_area']//10 == 5.0), ['room_count']] = 2\n",
    "df_all.loc[(df_all['apartment_id'] == 7118) & (df_all['supply_area']//10 == 5.0), ['bathroom_count']] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id'] == 4047) & (df_all['supply_area']//10 == 11.0), ['room_count']] = 3\n",
    "df_all.loc[(df_all['apartment_id'] == 4047) & (df_all['supply_area']//10 == 11.0), ['bathroom_count']] =2\n",
    "\n",
    "df_all.loc[(df_all['apartment_id'] == 37175) & (df_all['supply_area'] < 80), ['room_count']] = 2\n",
    "df_all.loc[(df_all['apartment_id'] == 37175) & (df_all['supply_area'] < 80), ['bathroom_count']] = 1\n",
    "\n",
    "df_all.loc[(df_all['apartment_id'] == 37175) & (df_all['supply_area'] > 80), ['room_count']] = 3\n",
    "df_all.loc[(df_all['apartment_id'] == 37175) & (df_all['supply_area'] > 80), ['bathroom_count']] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 주차장 결측치\n",
    "df_all.loc[(df_all['total_parking_capacity_in_site'].isnull()), ['total_parking_capacity_in_site']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 히트 결측치\n",
    "df_all.loc[(df_all['heat_type'].isnull()), ['heat_type']] = 'None'\n",
    "df_all.loc[(df_all['heat_fuel'].isnull()), ['heat_fuel']] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 현관구조 결측치\n",
    "df_all.loc[(df_all['heat_fuel'].isnull()), ['front_door_structure']] = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파생변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 용적률(容積率)은 건축 용어로 전체 대지면적에 대한 건물 연면적의 비율을 뜻하며 백분율로 표시한다. \n",
    "### 용적률이 높을수록 건축할 수 있는 연면적이 많아져 건축밀도가 높아지므로, 적정 주거환경을 보장하기 위하여 용적률의 상한선을 지정한다.\n",
    "df_all['effective_ratio'] = (df_all['exclusive_use_area'] / df_all['supply_area']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 시간을 좀 더 세부적으로 나타냄. \n",
    "df_all['transaction_date1'] = df_all.transaction_date.apply(lambda x: x[-2:])\n",
    "#convert int to date\n",
    "df_all['transaction_year_month1'] = df_all['transaction_year_month'].astype(str)\n",
    "#join month and date \n",
    "df_all['transaction_year_month_date'] = df_all[['transaction_year_month1', 'transaction_date1']].apply(lambda x: ''.join(x), axis=1)\n",
    "#convert  month and date to datetime \n",
    "df_all['transaction_year_month_date'] = pd.to_datetime(df_all['transaction_year_month_date'] )\n",
    "#reindext datetime\n",
    "del df_all['transaction_date1']; del df_all['transaction_year_month1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 계산의 편의성을 위해 날짜를 만들어 둠.\n",
    "df_all['year'] = df_all['transaction_year_month_date'].dt.year\n",
    "df_all['month'] = df_all['transaction_year_month_date'].dt.month\n",
    "df_all['month'] = df_all['month'].apply(lambda x : x-1 if x%2 == 0 else x)\n",
    "df_all['mean_year_month'] = df_all['year']*100 + df_all['month']\n",
    "del df_all['year']\n",
    "del df_all['month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 동일한 아파트의 가장 최근의 거래와 현재 거래의 차이를 계산\n",
    "df_all['last_month'] = df_all.groupby(['apartment_id'])['transaction_year_month'].shift(1)\n",
    "df_all['diff_month'] = df_all['transaction_year_month'] - df_all['last_month']\n",
    "del df_all['last_month']\n",
    "\n",
    "### 방의 총 갯수 ( 화장실 + 방 )\n",
    "df_all['total_room'] = df_all['room_count'] + df_all['bathroom_count']\n",
    "\n",
    "### Last_price_1과 3은 동일한 아파트의 면적대비 거래액을 의미. 추가로 현재 면적을 곱해줘야 함. \n",
    "df_all['last_price_1'] = df_all['transaction_real_price']/df_all['exclusive_use_area']\n",
    "df_all['last_price_1'] = df_all.groupby(['apartment_id'])['last_price_1'].shift(1)\n",
    "\n",
    "df_all['last_price_3'] = df_all['transaction_real_price']/df_all['supply_area']\n",
    "df_all['last_price_3'] = df_all.groupby(['apartment_id'])['last_price_3'].shift(1)\n",
    "\n",
    "df_all['last_area'] = df_all.groupby(['apartment_id'])['exclusive_use_area'].shift(1)\n",
    "df_all['last_transaction_year_month'] = df_all.groupby(['apartment_id'])['transaction_year_month'].shift(1)\n",
    "#df_all['transaction_real_price'] = np.log1p(df_all['transaction_real_price'])\n",
    "\n",
    "df_all['last_price_1'] = df_all['last_price_1'] * df_all['exclusive_use_area'] #현재 면적을 곱해줘서 비교를 가능하게 만듬.\n",
    "df_all['last_price_3'] = df_all['last_price_3'] * df_all['supply_area'] #현재 면적을 곱해줘서 비교를 가능하게 만듬.\n",
    "del df_all['last_area'],df_all['last_transaction_year_month']\n",
    "\n",
    "### log를 씌어줘서 정규성을 띄게 만듬. \n",
    "df_all['last_price_1'] = np.log1p(df_all['last_price_1'])\n",
    "df_all['last_price_3'] = np.log1p(df_all['last_price_3'])\n",
    "df_all['transaction_real_price'] = np.log1p(df_all['transaction_real_price'])\n",
    "\n",
    "### 빌딩의 간격계산\n",
    "df_all['difference_building_height'] = df_all['tallest_building_in_sites'] - df_all['lowest_building_in_sites']\n",
    "### 세대당 주차수 계산\n",
    "df_all['capacity_per_household'] = df_all['total_parking_capacity_in_site']/df_all['total_household_count_in_sites']\n",
    "\n",
    "### 아파트당 세대 수 계산\n",
    "df_all['household_per_building'] = df_all['total_household_count_in_sites']/df_all['apartment_building_count_in_sites']\n",
    "\n",
    "### 아파트당 타입의 비율 계산\n",
    "df_all['areahousehold_per_household'] = df_all['total_household_count_of_area_type']/df_all['total_household_count_in_sites']\n",
    "\n",
    "df_all['year'] = df_all['transaction_year_month']//100\n",
    "\n",
    "### 거래된 기간과 완성된 년도의 차이 계산\n",
    "df_all['transaction_diff_completion'] = df_all['transaction_year_month'] - df_all['year_of_completion']\n",
    "\n",
    "### 몇번째 층인지 비율 계산\n",
    "df_all['floor_ratio'] = df_all['floor']/df_all['tallest_building_in_sites']\n",
    "\n",
    "### 재개발 예정인지 가중치 줌. \n",
    "### 35를 상한선으로 잡은것은 이 이상이 되면 재개발 될 거라는 심리가 떨어져서 임.\n",
    "df_all['weight'] = 0\n",
    "df_all.loc[((df_all['year']-df_all['year_of_completion']) >= 25) & ((df_all['year']-df_all['year_of_completion']) < 35) & (df_all['effective_ratio'] >= 80) & (df_all['tallest_building_in_sites'] <=5),'weight'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subway, school 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 아래의 파일은 따로 첨부한 코드에 계산식이 나와있습니다.\n",
    "### Apartment_subway : 0.5, 1km내에 몇개의 지하철이 있냐, 몇개의 호선이 있냐\n",
    "### Apartment_gd_hd : 강남 및 해운대로부터의 거리가 얼마나 되냐\n",
    "### apartment_school : 0.5km 내에 초,중,고등학교의 갯수. total_0.5는 3개 중에서 몇개가 있는지\n",
    "### min_distance_apartment : 가장 가까운 초,중,고,지하철의 거리\n",
    "apartment = pd.read_csv(\"Apartment_subway.csv\")\n",
    "apartment1 = apartment[['apartment_id','subwayline_count_0.5','subwayline_count_1','subway_count_0.5','subway_count_1']]\n",
    "apartment2 = pd.read_csv(\"Apartment_ga_hd.csv\")\n",
    "apartment2 = apartment2[['apartment_id','gangnam_dist']]\n",
    "apartment3 = pd.read_csv(\"apartment_school.csv\")\n",
    "apartment3 = apartment3[['apartment_id','elementary_0.5','middle_0.5','high_0.5','total_0.5']]\n",
    "apartment4 = pd.read_csv(\"min_distance_apartment.csv\")\n",
    "apartment4 = apartment4[['apartment_id','subway_min_distance','min_distance_ele','min_distance_middle','min_distance_high']]\n",
    "#apartment4 = pd.read_csv(\"apartment_bub.csv\")\n",
    "#apartment4 = apartment4[['apartment_id','gu','dong']]\n",
    "df_all = pd.merge(df_all,apartment1,on='apartment_id').reset_index(drop=True)\n",
    "df_all = pd.merge(df_all,apartment2,on='apartment_id').reset_index(drop=True)\n",
    "df_all = pd.merge(df_all,apartment3,on='apartment_id').reset_index(drop=True)\n",
    "df_all = pd.merge(df_all,apartment4,on='apartment_id').reset_index(drop=True).sort_values('index') #index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 공공데이터\n",
    "- 사용목록과 코드는 따로 첨부하였습니다. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 쇼핑몰\n",
    "busan_shop = pd.read_csv(\"(0119)busan_shop.csv\")\n",
    "busan_shop = busan_shop[['apartment_id','shop_count_0.5','shop_count_1']]\n",
    "df_all = pd.merge(df_all,busan_shop,on='apartment_id').reset_index(drop=True).sort_values('index') #index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 대학교\n",
    "university = pd.read_csv('apartment_public.csv')\n",
    "university = university[['apartment_id','univ_1,2']]\n",
    "df_all = pd.merge(df_all,university,on='apartment_id').reset_index(drop=True).sort_values('index') #index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 구청\n",
    "public = pd.read_csv('apartment_public.csv')\n",
    "public = public[['apartment_id','public_1']]\n",
    "public['public_1'] = public['public_1'].apply(lambda x: 1 if x>1 else x)\n",
    "df_all = pd.merge(df_all,public,on='apartment_id').reset_index(drop=True).sort_values('index') #index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 스타벅스\n",
    "coffee = pd.read_csv(\"(0122)starbucks_list.csv\")\n",
    "coffee = coffee[['apartment_id','shop_count_1']]\n",
    "coffee.columns = ['apartment_id','coffee_count_1']\n",
    "df_all = pd.merge(df_all,coffee,on='apartment_id').reset_index(drop=True)\n",
    "#df_all['distance_from_hangang_6.0'] = df_all['distance_from_hangang_6.0'].apply(lambda x: 1 if x >=1 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA, FastICA,NMF,LatentDirichletAllocation,IncrementalPCA,MiniBatchSparsePCA\n",
    "from sklearn.decomposition import TruncatedSVD,FactorAnalysis,KernelPCA\n",
    "\n",
    "train_df = df_all.loc[df_all['transaction_real_price'] != 0]\n",
    "test_df = df_all.loc[df_all['transaction_real_price'] == 0]\n",
    "\n",
    "train_len = train_df.shape[0]\n",
    "\n",
    "### 날짜와 object, 공공데이터는 제거. \n",
    "train_columns = [c for c in train_df.columns if c not in ['key','transaction_real_price','transaction_year_month_date','transaction_date','heat_type','heat_fuel',\n",
    "                                                          'front_door_structure','shop_count_0.5','shop_count_1','univ_1,2','public_1','coffee_count_0.5']]\n",
    "train_columns\n",
    "\n",
    "# PCA\n",
    "n_comp = 1\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=2019)\n",
    "ica2_results_train = ica.fit_transform(train_df[train_columns].fillna(-1))\n",
    "ica2_results_test = ica.transform(test_df[train_columns].fillna(-1))\n",
    "\n",
    "for i in range(1, n_comp+1):\n",
    "    train_df['ica_' + str(i)] = ica2_results_train[:,i-1]\n",
    "    test_df['ica_' + str(i)] = ica2_results_test[:, i-1]\n",
    "    train_columns.append('ica_' + str(i))\n",
    "\n",
    "df_all = pd.concat([train_df,test_df])\n",
    "df_all = df_all.sort_values('index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 날짜 형식변경\n",
    "df_all['transaction_date1'] = df_all.transaction_date.apply(lambda x: x[-2:])\n",
    "#convert int to date\n",
    "df_all['transaction_year_month1'] = df_all['transaction_year_month'].astype(str)\n",
    "#join month and date \n",
    "df_all['transaction_year_month_date'] = df_all[['transaction_year_month1', 'transaction_date1']].apply(lambda x: ''.join(x), axis=1)\n",
    "df_all['transaction_year_month_date'] = df_all['transaction_year_month_date'].astype(int)\n",
    "del df_all['transaction_date1']; del df_all['transaction_year_month1']\n",
    "del df_all['transaction_year_month']; del df_all['transaction_date'] ; del df_all['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One-hot-encoding\n",
    "df_all = pd.get_dummies(df_all)\n",
    "train = df_all[:train_len]\n",
    "df_test = df_all[train_len:]\n",
    "del df_all\n",
    "train = train.sort_values('index')\n",
    "df_test = df_test.sort_values('index')\n",
    "\n",
    "### 거리의 경우 너무 크면 의미가 없어서 상관관계를 통해서 파악한 16을 기준으로 잘라버림. \n",
    "train['gangnam_dist'] = train['gangnam_dist'].apply(lambda x: 16 if x > 16 else x)\n",
    "df_test['gangnam_dist'] = df_test['gangnam_dist'].apply(lambda x: 16 if x > 16 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_features = [\n",
    "    'transaction_real_price'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'apartment_id'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['index']\n",
    "del df_test['index']\n",
    "del train['mean_year_month']\n",
    "del df_test['mean_year_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_busan = train[train['city']==0].reset_index(drop=True)\n",
    "df_test_busan = df_test[df_test['city']==0].reset_index(drop=True)\n",
    "df_train_seoul = train[train['city']==1].reset_index(drop=True)\n",
    "df_test_seoul = df_test[df_test['city']==1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 부산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "y_train = df_train_busan['transaction_real_price'].reset_index(drop=True)\n",
    "\n",
    "x_train = df_train_busan.copy().reset_index(drop=True)\n",
    "del x_train['city']; del x_train['transaction_real_price']; del x_train['public_1']; \n",
    "\n",
    "\n",
    "excluded_features = ['key']\n",
    "train_features = [_f for _f in x_train.columns if _f not in excluded_features]\n",
    "\n",
    "busan_key = df_test_busan['key'].values\n",
    "x_test = df_test_busan[train_features].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"/home/turbo/xgboost/python-package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb 0--------------------------------------------------\n",
      "[0]\ttrain-rmse:16.0789\tvalid-rmse:16.0773\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[1000]\ttrain-rmse:0.078053\tvalid-rmse:0.080297\n",
      "[2000]\ttrain-rmse:0.071209\tvalid-rmse:0.074712\n",
      "[3000]\ttrain-rmse:0.067747\tvalid-rmse:0.072371\n",
      "[4000]\ttrain-rmse:0.065348\tvalid-rmse:0.071051\n",
      "[5000]\ttrain-rmse:0.063432\tvalid-rmse:0.070162\n",
      "[6000]\ttrain-rmse:0.061822\tvalid-rmse:0.069546\n",
      "[7000]\ttrain-rmse:0.060421\tvalid-rmse:0.069071\n",
      "[8000]\ttrain-rmse:0.059198\tvalid-rmse:0.068724\n",
      "[9000]\ttrain-rmse:0.058095\tvalid-rmse:0.068469\n",
      "Stopping. Best iteration:\n",
      "[9047]\ttrain-rmse:0.058044\tvalid-rmse:0.068462\n",
      "\n",
      "xgb 1--------------------------------------------------\n",
      "[0]\ttrain-rmse:16.0785\tvalid-rmse:16.0787\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[1000]\ttrain-rmse:0.077946\tvalid-rmse:0.081113\n",
      "[2000]\ttrain-rmse:0.071085\tvalid-rmse:0.075524\n",
      "[3000]\ttrain-rmse:0.067607\tvalid-rmse:0.073223\n",
      "[4000]\ttrain-rmse:0.06517\tvalid-rmse:0.071916\n",
      "[5000]\ttrain-rmse:0.063313\tvalid-rmse:0.071092\n",
      "[6000]\ttrain-rmse:0.061727\tvalid-rmse:0.070477\n",
      "[7000]\ttrain-rmse:0.060382\tvalid-rmse:0.070011\n",
      "[8000]\ttrain-rmse:0.059157\tvalid-rmse:0.069646\n",
      "[9000]\ttrain-rmse:0.05807\tvalid-rmse:0.069355\n",
      "[10000]\ttrain-rmse:0.05706\tvalid-rmse:0.069133\n",
      "[11000]\ttrain-rmse:0.056168\tvalid-rmse:0.068957\n",
      "[12000]\ttrain-rmse:0.055315\tvalid-rmse:0.068803\n",
      "[13000]\ttrain-rmse:0.054533\tvalid-rmse:0.068666\n",
      "[14000]\ttrain-rmse:0.053789\tvalid-rmse:0.06856\n",
      "[15000]\ttrain-rmse:0.05305\tvalid-rmse:0.068471\n",
      "Stopping. Best iteration:\n",
      "[15047]\ttrain-rmse:0.053013\tvalid-rmse:0.06846\n",
      "\n",
      "xgb 2--------------------------------------------------\n",
      "[0]\ttrain-rmse:16.0792\tvalid-rmse:16.076\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[1000]\ttrain-rmse:0.077935\tvalid-rmse:0.080565\n",
      "[2000]\ttrain-rmse:0.071076\tvalid-rmse:0.075097\n",
      "[3000]\ttrain-rmse:0.067573\tvalid-rmse:0.072841\n",
      "[4000]\ttrain-rmse:0.065169\tvalid-rmse:0.07154\n",
      "[5000]\ttrain-rmse:0.063265\tvalid-rmse:0.070695\n",
      "[6000]\ttrain-rmse:0.061647\tvalid-rmse:0.070106\n",
      "[7000]\ttrain-rmse:0.06037\tvalid-rmse:0.069706\n",
      "[8000]\ttrain-rmse:0.059345\tvalid-rmse:0.069455\n",
      "[9000]\ttrain-rmse:0.058466\tvalid-rmse:0.069244\n",
      "Stopping. Best iteration:\n",
      "[9575]\ttrain-rmse:0.058157\tvalid-rmse:0.069183\n",
      "\n",
      "xgb 3--------------------------------------------------\n",
      "[0]\ttrain-rmse:16.0778\tvalid-rmse:16.0818\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[1000]\ttrain-rmse:0.078165\tvalid-rmse:0.080331\n",
      "[2000]\ttrain-rmse:0.071203\tvalid-rmse:0.074586\n",
      "[3000]\ttrain-rmse:0.067646\tvalid-rmse:0.07228\n",
      "[4000]\ttrain-rmse:0.065215\tvalid-rmse:0.071005\n",
      "[5000]\ttrain-rmse:0.063328\tvalid-rmse:0.070192\n",
      "[6000]\ttrain-rmse:0.061734\tvalid-rmse:0.069583\n",
      "[7000]\ttrain-rmse:0.06035\tvalid-rmse:0.069132\n",
      "[8000]\ttrain-rmse:0.059154\tvalid-rmse:0.068813\n",
      "[9000]\ttrain-rmse:0.058065\tvalid-rmse:0.068558\n",
      "[10000]\ttrain-rmse:0.057109\tvalid-rmse:0.068369\n",
      "[11000]\ttrain-rmse:0.05618\tvalid-rmse:0.068196\n",
      "[12000]\ttrain-rmse:0.05533\tvalid-rmse:0.068078\n",
      "Stopping. Best iteration:\n",
      "[12453]\ttrain-rmse:0.055161\tvalid-rmse:0.068049\n",
      "\n",
      "xgb 4--------------------------------------------------\n",
      "[0]\ttrain-rmse:16.0784\tvalid-rmse:16.079\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[1000]\ttrain-rmse:0.077969\tvalid-rmse:0.080552\n",
      "[2000]\ttrain-rmse:0.071025\tvalid-rmse:0.074967\n",
      "[3000]\ttrain-rmse:0.067467\tvalid-rmse:0.072667\n",
      "[4000]\ttrain-rmse:0.065052\tvalid-rmse:0.071416\n",
      "[5000]\ttrain-rmse:0.063229\tvalid-rmse:0.070653\n",
      "Stopping. Best iteration:\n",
      "[5536]\ttrain-rmse:0.062381\tvalid-rmse:0.070346\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17651645.4369657"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGB \n",
    "import time\n",
    "import xgboost as xgb\n",
    "#random_state =1 도전(기존 6)\n",
    "folds = KFold(n_splits=5,random_state=6,shuffle=True)\n",
    "oof_preds = np.zeros(x_train.shape[0])\n",
    "sub_preds = np.zeros(x_test.shape[0])\n",
    "\n",
    "start = time.time()\n",
    "valid_score = 0\n",
    "\n",
    "features_xgb = list(x_train.columns)\n",
    "feature_importance_df_xgb_deep = pd.DataFrame()\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(x_train, y_train)):\n",
    "    trn_x, trn_y = x_train[train_features].iloc[trn_idx], y_train[trn_idx]\n",
    "    val_x, val_y = x_train[train_features].iloc[val_idx], y_train[val_idx]    \n",
    "\n",
    "  \n",
    "    train_data = xgb.DMatrix(data=trn_x, label=trn_y)\n",
    "    valid_data = xgb.DMatrix(data=val_x, label=val_y)\n",
    "\n",
    "# fold 3, maxdepth 6, min_child_weight:0.513 -> 0.67(best)\n",
    "# fold3, maxdepth 5, minchild = 0.9 test -> 안좋음\n",
    "# maxdepth5, minchild:0.513 -> 테스트중 -> 구림\n",
    "# maxdepth 7, minchild:0.513, eta : 0.3\n",
    "    xgb_params = {'eta': 0.135, 'max_depth': 4, \n",
    "                  'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True,'min_child_weight':0.513, 'tree_learner':'gpu_exact'}\n",
    "\n",
    "    watchlist = [(train_data, 'train'), (valid_data, 'valid')]\n",
    "    print(\"xgb \" + str(n_fold) + \"-\" * 50)\n",
    "\n",
    "    num_round = 20000\n",
    "    xgb_model = xgb.train(xgb_params, train_data, num_round, watchlist, early_stopping_rounds=100, verbose_eval=1000)\n",
    "    oof_preds[val_idx] = xgb_model.predict(xgb.DMatrix(val_x), ntree_limit=xgb_model.best_ntree_limit+50)\n",
    "    oof_preds[oof_preds<0] = 0\n",
    "    sub_pred = xgb_model.predict(xgb.DMatrix(x_test[train_features]), ntree_limit=xgb_model.best_ntree_limit+50) / 5\n",
    "    sub_pred[sub_pred<0] = 0 \n",
    "    sub_preds += sub_pred\n",
    "\n",
    "          #print('Fold %2d rmse : %.6f' % (n_fold + 1, np.sqrt(mean_squared_error(val_y, oof_preds[val_idx]))))\n",
    "    valid_score += mean_squared_error(val_y, oof_preds[val_idx])\n",
    "\n",
    "    result = xgb_model.get_fscore()\n",
    "    fold_importance_df_xgb_deep = pd.DataFrame(pd.Series(result), columns=['importance']).reset_index()\n",
    "    feature_importance_df_xgb_deep = pd.concat([feature_importance_df_xgb_deep, fold_importance_df_xgb_deep], axis=0)\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "np.sqrt(mean_squared_error(np.expm1(y_train), np.expm1(oof_preds))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['transaction_real_price'] = oof_preds\n",
    "x_train.to_csv(\"[190130-2]XGB_Bestmodel_busan_train.csv\",index=False)\n",
    "\n",
    "x_test['transaction_real_price'] = sub_preds \n",
    "x_test['key'] = busan_key\n",
    "x_test.to_csv(\"[190130-2]XGB_Bestmodel_busan_test.csv\",index=False)\n",
    "\n",
    "sub_busan_not = x_test[['key','transaction_real_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full rmse score 24542623.109343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "busan_valid.columns = ['key','valid_price']\n",
    "busan_valid = pd.merge(busan_valid,x_train,on='key',how='left')\n",
    "\n",
    "print('Full rmse score %.6f\\n' % np.sqrt(mean_squared_error(np.expm1(busan_valid['transaction_real_price']), busan_valid['valid_price'])))\n",
    "busan_score_not = np.sqrt(mean_squared_error(np.expm1(busan_valid['transaction_real_price']), busan_valid['valid_price']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 서울"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "y_train = df_train_seoul['transaction_real_price'].reset_index(drop=True)\n",
    "\n",
    "x_train = df_train_seoul.copy().reset_index(drop=True)\n",
    "del x_train['city']; del x_train['transaction_real_price']; del x_train['last_price_3'];  del x_train['ica_1']\n",
    "\n",
    "\n",
    "excluded_features = ['key','floor']\n",
    "train_features = [_f for _f in x_train.columns if _f not in excluded_features]\n",
    "\n",
    "seoul_key = df_test_seoul['key'].values\n",
    "seoul_floor = df_test_seoul['floor'].values\n",
    "x_test = df_test_seoul[train_features].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb 0--------------------------------------------------\n",
      "[0]\ttrain-rmse:16.7488\tvalid-rmse:16.7499\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 90 rounds.\n",
      "[1000]\ttrain-rmse:0.067579\tvalid-rmse:0.071536\n",
      "[2000]\ttrain-rmse:0.060274\tvalid-rmse:0.066618\n",
      "[3000]\ttrain-rmse:0.056329\tvalid-rmse:0.064741\n",
      "[4000]\ttrain-rmse:0.053543\tvalid-rmse:0.063704\n",
      "[5000]\ttrain-rmse:0.051467\tvalid-rmse:0.063071\n",
      "[6000]\ttrain-rmse:0.049687\tvalid-rmse:0.062711\n",
      "[7000]\ttrain-rmse:0.048189\tvalid-rmse:0.062375\n",
      "[8000]\ttrain-rmse:0.046872\tvalid-rmse:0.062146\n",
      "[9000]\ttrain-rmse:0.045662\tvalid-rmse:0.061954\n",
      "[10000]\ttrain-rmse:0.04461\tvalid-rmse:0.061791\n",
      "[11000]\ttrain-rmse:0.043653\tvalid-rmse:0.06166\n",
      "[12000]\ttrain-rmse:0.042789\tvalid-rmse:0.061564\n",
      "Stopping. Best iteration:\n",
      "[12064]\ttrain-rmse:0.042734\tvalid-rmse:0.061559\n",
      "\n",
      "xgb 1--------------------------------------------------\n",
      "[0]\ttrain-rmse:16.749\tvalid-rmse:16.7492\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 90 rounds.\n",
      "[1000]\ttrain-rmse:0.06755\tvalid-rmse:0.071628\n",
      "[2000]\ttrain-rmse:0.060337\tvalid-rmse:0.066789\n",
      "[3000]\ttrain-rmse:0.056364\tvalid-rmse:0.065014\n",
      "[4000]\ttrain-rmse:0.053609\tvalid-rmse:0.064116\n",
      "[5000]\ttrain-rmse:0.051465\tvalid-rmse:0.063521\n",
      "[6000]\ttrain-rmse:0.049734\tvalid-rmse:0.06313\n",
      "[7000]\ttrain-rmse:0.048225\tvalid-rmse:0.062772\n",
      "[8000]\ttrain-rmse:0.046896\tvalid-rmse:0.062576\n",
      "[9000]\ttrain-rmse:0.04571\tvalid-rmse:0.062428\n",
      "Stopping. Best iteration:\n",
      "[9713]\ttrain-rmse:0.044942\tvalid-rmse:0.062322\n",
      "\n",
      "xgb 2--------------------------------------------------\n",
      "[0]\ttrain-rmse:16.7488\tvalid-rmse:16.7497\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 90 rounds.\n",
      "[1000]\ttrain-rmse:0.067445\tvalid-rmse:0.071756\n",
      "[2000]\ttrain-rmse:0.060143\tvalid-rmse:0.066779\n",
      "[3000]\ttrain-rmse:0.056249\tvalid-rmse:0.064926\n",
      "[4000]\ttrain-rmse:0.053567\tvalid-rmse:0.063976\n",
      "[5000]\ttrain-rmse:0.051451\tvalid-rmse:0.063366\n",
      "[6000]\ttrain-rmse:0.049685\tvalid-rmse:0.062909\n",
      "[7000]\ttrain-rmse:0.048201\tvalid-rmse:0.062601\n",
      "[8000]\ttrain-rmse:0.04687\tvalid-rmse:0.062331\n",
      "[9000]\ttrain-rmse:0.045699\tvalid-rmse:0.062151\n",
      "Stopping. Best iteration:\n",
      "[9639]\ttrain-rmse:0.044993\tvalid-rmse:0.062054\n",
      "\n",
      "xgb 3--------------------------------------------------\n",
      "[0]\ttrain-rmse:16.7497\tvalid-rmse:16.7462\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 90 rounds.\n",
      "[1000]\ttrain-rmse:0.067451\tvalid-rmse:0.072247\n",
      "[2000]\ttrain-rmse:0.060115\tvalid-rmse:0.067437\n",
      "[3000]\ttrain-rmse:0.056239\tvalid-rmse:0.065551\n",
      "[4000]\ttrain-rmse:0.053594\tvalid-rmse:0.064561\n",
      "[5000]\ttrain-rmse:0.051469\tvalid-rmse:0.063939\n",
      "[6000]\ttrain-rmse:0.049665\tvalid-rmse:0.063536\n",
      "[7000]\ttrain-rmse:0.048164\tvalid-rmse:0.063253\n",
      "[8000]\ttrain-rmse:0.04678\tvalid-rmse:0.063013\n",
      "Stopping. Best iteration:\n",
      "[8645]\ttrain-rmse:0.046011\tvalid-rmse:0.062908\n",
      "\n",
      "xgb 4--------------------------------------------------\n",
      "[0]\ttrain-rmse:16.7488\tvalid-rmse:16.75\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 90 rounds.\n",
      "[1000]\ttrain-rmse:0.067808\tvalid-rmse:0.070126\n",
      "[2000]\ttrain-rmse:0.060489\tvalid-rmse:0.065116\n",
      "[3000]\ttrain-rmse:0.056522\tvalid-rmse:0.063228\n",
      "[4000]\ttrain-rmse:0.053825\tvalid-rmse:0.062216\n",
      "[5000]\ttrain-rmse:0.051709\tvalid-rmse:0.061587\n",
      "[6000]\ttrain-rmse:0.049904\tvalid-rmse:0.061153\n",
      "[7000]\ttrain-rmse:0.048381\tvalid-rmse:0.060841\n",
      "[8000]\ttrain-rmse:0.047065\tvalid-rmse:0.060614\n",
      "[9000]\ttrain-rmse:0.045893\tvalid-rmse:0.060435\n",
      "Stopping. Best iteration:\n",
      "[9685]\ttrain-rmse:0.045146\tvalid-rmse:0.060317\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37452203.889437295"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import xgboost as xgb\n",
    "\n",
    "#random_state = 1 도전 (기존 6)\n",
    "folds = KFold(n_splits=5,random_state=6,shuffle=True)\n",
    "oof_preds = np.zeros(x_train.shape[0])\n",
    "sub_preds = np.zeros(x_test.shape[0])\n",
    "\n",
    "start = time.time()\n",
    "valid_score = 0\n",
    "\n",
    "features_xgb = list(x_train.columns)\n",
    "feature_importance_df_xgb_deep = pd.DataFrame()\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(x_train, y_train)):\n",
    "    trn_x, trn_y = x_train[train_features].iloc[trn_idx], y_train[trn_idx]\n",
    "    val_x, val_y = x_train[train_features].iloc[val_idx], y_train[val_idx]    \n",
    "\n",
    "  \n",
    "    train_data = xgb.DMatrix(data=trn_x, label=trn_y)\n",
    "    valid_data = xgb.DMatrix(data=val_x, label=val_y)\n",
    "    # 5, 0.9로 바꿈 -> 3700\n",
    "    # 6, 0.9 실험 -> ?\n",
    "    xgb_params = {'eta': 0.135, 'max_depth':5, \n",
    "                  'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True,'min_child_weight':0.9, 'tree_learner':'gpu_exact'}\n",
    "\n",
    "    watchlist = [(train_data, 'train'), (valid_data, 'valid')]\n",
    "    print(\"xgb \" + str(n_fold) + \"-\" * 50)\n",
    "\n",
    "    num_round = 20000\n",
    "    xgb_model = xgb.train(xgb_params, train_data, num_round, watchlist, early_stopping_rounds=90, verbose_eval=1000)\n",
    "    oof_preds[val_idx] = xgb_model.predict(xgb.DMatrix(val_x), ntree_limit=xgb_model.best_ntree_limit+50)\n",
    "    oof_preds[oof_preds<0] = 0\n",
    "    sub_pred = xgb_model.predict(xgb.DMatrix(x_test[train_features]), ntree_limit=xgb_model.best_ntree_limit+50) / 5\n",
    "    sub_pred[sub_pred<0] = 0 \n",
    "    sub_preds += sub_pred\n",
    "\n",
    "          #print('Fold %2d rmse : %.6f' % (n_fold + 1, np.sqrt(mean_squared_error(val_y, oof_preds[val_idx]))))\n",
    "    valid_score += mean_squared_error(val_y, oof_preds[val_idx])\n",
    "\n",
    "    result = xgb_model.get_fscore()\n",
    "    fold_importance_df_xgb_deep = pd.DataFrame(pd.Series(result), columns=['importance']).reset_index()\n",
    "    feature_importance_df_xgb_deep = pd.concat([feature_importance_df_xgb_deep, fold_importance_df_xgb_deep], axis=0)\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "np.sqrt(mean_squared_error(np.expm1(y_train), np.expm1(oof_preds))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['transaction_real_price'] = oof_preds\n",
    "x_train.to_csv(\"[190130-2]XGB_Bestmodel_seoul_train.csv\",index=False)\n",
    "\n",
    "x_test['transaction_real_price'] = sub_preds\n",
    "x_test['key'] = seoul_key\n",
    "x_test['floor'] = seoul_floor\n",
    "x_test.to_csv(\"[190130-2]XGB_Bestmodel_seoul_test.csv\",index=False)\n",
    "\n",
    "sub_seoul_not = x_test[['key','transaction_real_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full rmse score 64096517.142327\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seoul_valid.columns = ['key','valid_price']\n",
    "seoul_valid = pd.merge(seoul_valid,x_train,on='key',how='left')\n",
    "\n",
    "print('Full rmse score %.6f\\n' % np.sqrt(mean_squared_error(np.expm1(seoul_valid['transaction_real_price']), seoul_valid['valid_price'])))\n",
    "seoul_score_not = np.sqrt(mean_squared_error(np.expm1(seoul_valid['transaction_real_price']), seoul_valid['valid_price']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sub_busan_not['transaction_real_price'].min() > 100:\n",
    "    sub_busan_not['transaction_real_price'] = np.log1p(sub_busan_not['transaction_real_price'])\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>transaction_real_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1253422</td>\n",
       "      <td>18.516604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1369751</td>\n",
       "      <td>18.781540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1389544</td>\n",
       "      <td>18.595687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1394472</td>\n",
       "      <td>19.829742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1395869</td>\n",
       "      <td>19.324720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       key  transaction_real_price\n",
       "0  1253422               18.516604\n",
       "1  1369751               18.781540\n",
       "2  1389544               18.595687\n",
       "3  1394472               19.829742\n",
       "4  1395869               19.324720"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_not_deep = pd.concat([sub_busan_not,sub_seoul_not])\n",
    "sub_not_deep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_not_deep['transaction_real_price'] = np.expm1(sub_not_deep['transaction_real_price'])\n",
    "sub_not_deep = sub_not_deep.sort_values('key')\n",
    "sub_not_deep = sub_not_deep.reset_index(drop=True)\n",
    "sub_not_deep.to_csv(\"01.30.XGB_Bestscore-2.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
