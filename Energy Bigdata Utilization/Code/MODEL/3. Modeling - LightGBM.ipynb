{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook \n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/choco/Desktop/Github/Dacon/Energy Bigdata Utilization/'\n",
    "train = pd.read_csv(path + \"input/train2.csv\")\n",
    "test = pd.read_csv(path + \"input/test2.csv\")\n",
    "submission = pd.read_csv(path + \"input/submission_1002.csv\")\n",
    "test_df = pd.read_csv(path + \"input/test_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "sub_df = pd.DataFrame()\n",
    "date_list_day = [datetime.datetime(2018,7,1,0) + datetime.timedelta(hours=x) for x in range(24)]\n",
    "date_list = []\n",
    "meter_id = []\n",
    "for i in tqdm_notebook(test_df['meter_id'].unique()):\n",
    "    date_list = np.append(date_list, date_list_day)\n",
    "    meter_id = np.append(meter_id, np.array([i] * 24))\n",
    "\n",
    "sub_df['time'] = date_list\n",
    "sub_df['meter_id'] = meter_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "sub_df_day = pd.DataFrame()\n",
    "date_list_day = [datetime.datetime(2018,7,1,0) + datetime.timedelta(hours=x) for x in range(24*10)]\n",
    "date_list = []\n",
    "meter_id = []\n",
    "for i in tqdm_notebook(test_df['meter_id'].unique()):\n",
    "    date_list = np.append(date_list, date_list_day)\n",
    "    meter_id = np.append(meter_id, np.array([i] * 24 * 10))\n",
    "\n",
    "sub_df_day['time'] = date_list\n",
    "sub_df_day['meter_id'] = meter_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "sub_df_month = pd.DataFrame()\n",
    "date_list_month = [datetime.datetime(2018,7,1,0) + datetime.timedelta(hours=x) for x in range(24 * 153)]\n",
    "date_list = []\n",
    "meter_id = []\n",
    "for i in tqdm_notebook(test_df['meter_id'].unique()):\n",
    "    date_list = np.append(date_list, date_list_month)\n",
    "    meter_id = np.append(meter_id, np.array([i] * 24 * 153))\n",
    "\n",
    "sub_df_month['time'] = date_list\n",
    "sub_df_month['meter_id'] = meter_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [test_df, sub_df]:\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['year'] = df['time'].dt.year\n",
    "    df['month'] = df['time'].dt.month\n",
    "    df['day'] = df['time'].dt.day\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['weekday'] = df['time'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([test_df, sub_df], axis=0, sort=False)\n",
    "all_df['y_hr'] = all_df.groupby(['meter_id'])['y'].shift(24)\n",
    "\n",
    "test_df_hour = all_df[:test_df.shape[0]]\n",
    "sub_df_hour = all_df[test_df.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([test_df, sub_df_day], axis=0, sort=False)\n",
    "all_df['y_hr'] = all_df.groupby(['meter_id'])['y'].shift(24 * 10)\n",
    "\n",
    "test_df_day = all_df[:test_df.shape[0]]\n",
    "sub_df_day = all_df[test_df.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([test_df, sub_df_month], axis=0, sort=False)\n",
    "all_df['y_hr'] = all_df.groupby(['meter_id'])['y'].shift(24 * 153)\n",
    "\n",
    "test_df_month = all_df[:test_df.shape[0]]\n",
    "sub_df_month = all_df[test_df.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_df; \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External data - weather "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "weather = pd.read_csv(\"./input/weather.csv\", encoding='cp949')[['일시', '기온(°C)', '풍속(m/s)']]\n",
    "weather['일시'] = pd.to_datetime(weather['일시'])\n",
    "weather.rename(columns={'일시':'time'},inplace=True)\n",
    "\n",
    "\n",
    "test_df_hour = test_df_hour.merge(weather, how='left', on='time')\n",
    "test_df_day = test_df_day.merge(weather, how='left', on='time')\n",
    "test_df_month = test_df_month.merge(weather, how='left', on='time')\n",
    "\n",
    "\n",
    "sub_df_hour = sub_df_hour.merge(weather, how='left', on='time')\n",
    "sub_df_day = sub_df_day.merge(weather, how='left', on='time')\n",
    "sub_df_month = sub_df_month.merge(weather, how='left', on='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hour = test_df[test_df['time'] < '2018-06-30 00:00:00'].reset_index(drop=True)\n",
    "valid_hour = test_df[test_df['time'] >= '2018-06-30 00:00:00'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_day = test_df_day[test_df_day['time'] < '2018-06-21 00:00:00'].reset_index(drop=True)\n",
    "valid_day = test_df_day[test_df_day['time'] >= '2018-06-21 00:00:00'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_month = test_df_month[test_df_month['time'] < '2018-01-01 00:00:00'].reset_index(drop=True)\n",
    "valid_month = test_df_month[test_df_month['time'] >= '2018-01-01 00:00:00'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM - Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 20,\n",
    "          'objective': 'regression',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.001,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mape',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.3899927210061127,\n",
    "          'reg_lambda': 0.6485237330340494,\n",
    "          'random_state': 47\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = [c for c in train_hour.columns if c not in ['time','y']]\n",
    " \n",
    "\n",
    "y = train_hour['y']\n",
    "y_valid = valid_hour['y']\n",
    "\n",
    "X = train_hour[features].reset_index(drop=True)\n",
    "V = valid_hour[features].reset_index(drop=True)\n",
    "sub = sub_df_hour[features].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in tqdm_notebook(['meter_id']):\n",
    "    if X[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(X[col].astype(str).values) + list(sub[col].astype(str).values))\n",
    "        X[col] = le.transform(list(X[col].astype(str).values))\n",
    "        sub[col] = le.transform(list(sub[col].astype(str).values))   \n",
    "        V[col] = le.transform(list(V[col].astype(str).values))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "mape = list()\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = X.columns\n",
    "\n",
    "training_start_time = time()\n",
    "start_time = time()\n",
    "    \n",
    "trn_data = lgb.Dataset(X, label=y, categorical_feature = ['meter_id'])\n",
    "val_data = lgb.Dataset(V, label=y_valid, categorical_feature = ['meter_id'])\n",
    "clf = lgb.train(params, trn_data, 2500, valid_sets = [trn_data, val_data], \n",
    "                    verbose_eval=100, early_stopping_rounds=50)\n",
    "    \n",
    "mape.append(clf.best_score['valid_1']['mape'])\n",
    "    \n",
    "print('-' * 30)\n",
    "print('Training has finished.')\n",
    "print('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n",
    "print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_iter = clf.best_iteration\n",
    "clf = lgb.LGBMRegressor(**params, num_boost_round=best_iter)\n",
    "clf.fit(pd.concat([X,V],axis=0), pd.concat([y,y_valid],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_hour['y'] = clf.predict(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_hour = pd.DataFrame()\n",
    "meter_id_list = []\n",
    "output_list = []\n",
    "for meter_id in tqdm_notebook(sub_df_hour['meter_id'].unique()):\n",
    "    meter_id_list.append(meter_id)\n",
    "    output_list.append(sub_df_hour[sub_df_hour['meter_id']==meter_id]['y'].values)\n",
    "    \n",
    "submission_hour['merter_id'] = meter_id_list\n",
    "submission_hour = pd.concat([submission_hour, pd.DataFrame(output_list)], axis=1)\n",
    "submission_hour.columns = np.append(['meter_id'], ['X2018_7_1_'+str(i+1)+'h' for i in range(24)])\n",
    "submission_hour['meter_id_2'] = submission_hour['meter_id'].apply(lambda x: x[1:]).astype(int)\n",
    "submission_hour = submission_hour.sort_values(by='meter_id_2')\n",
    "del submission_hour['meter_id_2']\n",
    "submission_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM - day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "\n",
    "aggs = {}\n",
    "aggs['y'] = ['sum']\n",
    "aggs['y_hr'] = ['sum','max','min','mean','var']\n",
    "#aggs['기온(°C)'] = ['max','min','mean','var']\n",
    "#aggs['풍속(m/s)'] = ['max','min','mean','var']\n",
    "\n",
    "new_columns = create_new_columns('day',aggs)\n",
    "train_day['date'] = train_day['time'].dt.date\n",
    "train_day_group_df = train_day.groupby(['date','meter_id']).agg(aggs)\n",
    "train_day_group_df.columns = new_columns\n",
    "train_day_group_df.reset_index(drop=False,inplace=True)\n",
    "\n",
    "valid_day['date'] = valid_day['time'].dt.date\n",
    "valid_day_group_df = valid_day.groupby(['date','meter_id']).agg(aggs)\n",
    "valid_day_group_df.columns = new_columns\n",
    "valid_day_group_df.reset_index(drop=False,inplace=True)\n",
    "\n",
    "sub_df_day['date'] = sub_df_day['time'].dt.date\n",
    "sub_df_day_group_df = sub_df_day.groupby(['date','meter_id']).agg(aggs)\n",
    "sub_df_day_group_df.columns = new_columns\n",
    "sub_df_day_group_df.reset_index(drop=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = [c for c in train_day_group_df.columns if c not in ['date','y','day_y_sum']]\n",
    " \n",
    "\n",
    "y = train_day_group_df['day_y_sum']\n",
    "y_valid = valid_day_group_df['day_y_sum']\n",
    "\n",
    "X = train_day_group_df[features].reset_index(drop=True)\n",
    "V = valid_day_group_df[features].reset_index(drop=True)\n",
    "sub = sub_df_day_group_df[features].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in tqdm_notebook(['meter_id']):\n",
    "    if X[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(X[col].astype(str).values) + list(sub[col].astype(str).values))\n",
    "        X[col] = le.transform(list(X[col].astype(str).values))\n",
    "        sub[col] = le.transform(list(sub[col].astype(str).values))   \n",
    "        V[col] = le.transform(list(V[col].astype(str).values))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "mape = list()\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = X.columns\n",
    "\n",
    "training_start_time = time()\n",
    "start_time = time()\n",
    "    \n",
    "trn_data = lgb.Dataset(X, label=y, categorical_feature = ['meter_id'])\n",
    "val_data = lgb.Dataset(V, label=y_valid, categorical_feature = ['meter_id'])\n",
    "clf = lgb.train(params, trn_data, 2500, valid_sets = [trn_data, val_data], \n",
    "                    verbose_eval=100, early_stopping_rounds=50)\n",
    "    \n",
    "mape.append(clf.best_score['valid_1']['mape'])\n",
    "    \n",
    "print('-' * 30)\n",
    "print('Training has finished.')\n",
    "print('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n",
    "print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iter = clf.best_iteration\n",
    "clf = lgb.LGBMRegressor(**params, num_boost_round=best_iter)\n",
    "clf.fit(pd.concat([X,V],axis=0), pd.concat([y,y_valid],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_day_group_df['y'] = clf.predict(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_day = pd.DataFrame()\n",
    "meter_id_list = []\n",
    "output_list = []\n",
    "for meter_id in tqdm_notebook(sub_df_day_group_df['meter_id'].unique()):\n",
    "    meter_id_list.append(meter_id)\n",
    "    output_list.append(sub_df_day_group_df[sub_df_day_group_df['meter_id']==meter_id]['y'].values)\n",
    "    \n",
    "submission_day['merter_id'] = meter_id_list\n",
    "submission_day = pd.concat([submission_day, pd.DataFrame(output_list)], axis=1)\n",
    "submission_day.columns = np.append(['meter_id'], ['X2018_7_'+str(i+1)+'_d' for i in range(10)])\n",
    "submission_day['meter_id_2'] = submission_day['meter_id'].apply(lambda x: x[1:]).astype(int)\n",
    "submission_day = submission_day.sort_values(by='meter_id_2').reset_index(drop=True)\n",
    "del submission_day['meter_id_2']\n",
    "submission_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM - month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "\n",
    "aggs = {}\n",
    "aggs['y'] = ['sum']\n",
    "aggs['y_hr'] = ['sum','max','min','mean','var']\n",
    "#aggs['기온(°C)'] = ['max','min','mean','var']\n",
    "#aggs['풍속(m/s)'] = ['max','min','mean','var']\n",
    "\n",
    "new_columns = create_new_columns('month',aggs)\n",
    "train_month_group_df = train_month.groupby(['year_month','meter_id']).agg(aggs)\n",
    "train_month_group_df.columns = new_columns\n",
    "train_month_group_df.reset_index(drop=False,inplace=True)\n",
    "\n",
    "valid_month_group_df = valid_month.groupby(['year_month','meter_id']).agg(aggs)\n",
    "valid_month_group_df.columns = new_columns\n",
    "valid_month_group_df.reset_index(drop=False,inplace=True)\n",
    "\n",
    "sub_df_month_group_df = sub_df_month.groupby(['year_month','meter_id']).agg(aggs)\n",
    "sub_df_month_group_df.columns = new_columns\n",
    "sub_df_month_group_df.reset_index(drop=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in train_month_group_df.columns if c not in ['date','y','month_y_sum','year_month']]\n",
    " \n",
    "\n",
    "y = train_month_group_df['month_y_sum']\n",
    "y_valid = valid_month_group_df['month_y_sum']\n",
    "\n",
    "X = train_month_group_df[features].reset_index(drop=True)\n",
    "V = valid_month_group_df[features].reset_index(drop=True)\n",
    "sub = sub_df_month_group_df[features].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in tqdm_notebook(['meter_id']):\n",
    "    if X[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(X[col].astype(str).values) + list(sub[col].astype(str).values))\n",
    "        X[col] = le.transform(list(X[col].astype(str).values))\n",
    "        sub[col] = le.transform(list(sub[col].astype(str).values))   \n",
    "        V[col] = le.transform(list(V[col].astype(str).values))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 20,\n",
    "          'objective': 'regression',\n",
    "          'max_depth': 3,\n",
    "          'learning_rate': 0.0001,\n",
    "          \"boosting_type\": \"dart\",\n",
    "          \"metric\": 'mape',\n",
    "          \"verbosity\": -1,\n",
    "          'random_state': 47\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "mape = list()\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = X.columns\n",
    "\n",
    "training_start_time = time()\n",
    "start_time = time()\n",
    "    \n",
    "trn_data = lgb.Dataset(X, label=y, categorical_feature = ['meter_id'])\n",
    "val_data = lgb.Dataset(V, label=y_valid, categorical_feature = ['meter_id'])\n",
    "clf = lgb.train(params, trn_data, 2500, valid_sets = [trn_data, val_data], \n",
    "                    verbose_eval=100, early_stopping_rounds=50)\n",
    "    \n",
    "mape.append(clf.best_score['valid_1']['mape'])\n",
    "    \n",
    "print('-' * 30)\n",
    "print('Training has finished.')\n",
    "print('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n",
    "print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iter = clf.best_iteration\n",
    "clf = lgb.LGBMRegressor(**params, num_boost_round=best_iter)\n",
    "clf.fit(pd.concat([X,V],axis=0), pd.concat([y,y_valid],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_month_group_df['y'] = clf.predict(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_month = pd.DataFrame()\n",
    "meter_id_list = []\n",
    "output_list = []\n",
    "for meter_id in tqdm_notebook(sub_df_month_group_df['meter_id'].unique()):\n",
    "    meter_id_list.append(meter_id)\n",
    "    output_list.append(sub_df_month_group_df[sub_df_month_group_df['meter_id']==meter_id]['y'].values)\n",
    "    \n",
    "submission_month['merter_id'] = meter_id_list\n",
    "submission_month = pd.concat([submission_month, pd.DataFrame(output_list)], axis=1)\n",
    "submission_month.columns = np.append(['meter_id'], ['X2018_'+str(i+7)+'_m' for i in range(5)])\n",
    "submission_month['meter_id_2'] = submission_month['meter_id'].apply(lambda x: x[1:]).astype(int)\n",
    "submission_month = submission_month.sort_values(by='meter_id_2').reset_index(drop=True)\n",
    "del submission_month['meter_id_2']\n",
    "submission_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_total = pd.merge(submission_hour, submission_day, how='left', on='meter_id')\n",
    "submission_total = pd.merge(submission_total, submission_month, how='left', on='meter_id')\n",
    "submission_total.to_csv(\"submission_total.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
